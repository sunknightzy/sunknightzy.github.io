[{"id":"0cfdc023c234c1bc698e1aac4bca057c","title":"JetBrains全家桶体验方案","content":"下载ToolBox访问 JetBrains官网 下载安装 toolbox ，然后从 toolbox 中把自己需要的 IDE 安装好\n下载Agent工具访问 Agent网站 ，会自动检测出可以访问的域名，点击一个可访问的域名，然后下载 jetbra.zip 文件，解压到 home 目录下，执行下面的命令\nbash# 卸载旧版 agent\nsudo sh ~/jetbra/script/uninstall.sh\n\n# 安装新版 agent\nsudo sh ~/jetbra/script/install.shinstall.sh 就是给电脑添加了一个自动后台运行的启动项  jetbrains.vmoptions.plist\n\n 查看 install.sh 内容\nxml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot;\n    &quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt;\n&lt;plist version=&quot;1.0&quot;&gt;\n  &lt;dict&gt;\n    &lt;key&gt;Label&lt;/key&gt;\n    &lt;string&gt;jetbrains.vmoptions&lt;/string&gt;\n    &lt;key&gt;ProgramArguments&lt;/key&gt;\n    &lt;array&gt;\n      &lt;string&gt;sh&lt;/string&gt;\n      &lt;string&gt;-c&lt;/string&gt;\n      &lt;string&gt;\n        launchctl setenv &quot;IDEA_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/idea.vmoptions&quot;\n        launchctl setenv &quot;CLION_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/clion.vmoptions&quot;\n        launchctl setenv &quot;PHPSTORM_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/phpstorm.vmoptions&quot;\n        launchctl setenv &quot;GOLAND_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/goland.vmoptions&quot;\n        launchctl setenv &quot;PYCHARM_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/pycharm.vmoptions&quot;\n        launchctl setenv &quot;WEBSTORM_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/webstorm.vmoptions&quot;\n        launchctl setenv &quot;WEBIDE_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/webide.vmoptions&quot;\n        launchctl setenv &quot;RIDER_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/rider.vmoptions&quot;\n        launchctl setenv &quot;DATAGRIP_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/datagrip.vmoptions&quot;\n        launchctl setenv &quot;RUBYMINE_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/rubymine.vmoptions&quot;\n        launchctl setenv &quot;DATASPELL_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/dataspell.vmoptions&quot;\n        launchctl setenv &quot;AQUA_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/aqua.vmoptions&quot;\n        launchctl setenv &quot;RUSTROVER_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/rustrover.vmoptions&quot;\n        launchctl setenv &quot;GATEWAY_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/gateway.vmoptions&quot;\n        launchctl setenv &quot;JETBRAINS_CLIENT_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/jetbrains_client.vmoptions&quot;\n        launchctl setenv &quot;JETBRAINSCLIENT_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/jetbrainsclient.vmoptions&quot;\n        launchctl setenv &quot;STUDIO_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/studio.vmoptions&quot;\n        launchctl setenv &quot;DEVECOSTUDIO_VM_OPTIONS&quot; &quot;/Users/oak/jetbra/vmoptions/devecostudio.vmoptions&quot;\n      &lt;/string&gt;\n    &lt;/array&gt;\n    &lt;key&gt;RunAtLoad&lt;/key&gt;\n    &lt;true/&gt;\n  &lt;/dict&gt;\n&lt;/plist&gt;\n\n这个启动项用于设置环境变量，替换全家桶中 IDE 的启动参数，所以解压后的 agent 目录不要去移动，否则 IDE 会要求重新激活，一般情况下，到这里就 OK 了，直接去复制体验码过来贴进去就可以了\n如果还无法成功，就需要手动设置 IDE 启动参数了\n手动修改IDE的启动参数以 IDEA 为例：\n1、打开 toolbox ，编辑 IDEA 的 JVM 启动参数\n \n \n2、把 jetbra&#x2F;vmoptions&#x2F;idea.vmoptions 文件中的内容直接复制到 IDEA 的 JVM options 里面（里面的内容可以自己改一下，比如内存大小），不要覆盖里面关于 toolbox 的部分\n3、打开 IDEA，会弹出激活界面，选择 Active Code 激活，体验码从下载 Agent 的网站直接复制过来\n到此就可以尽情体验 IDEA 了，其它的 IDE 如法炮制\n","slug":"JetBrains全家桶体验方案","date":"2022-03-31T05:36:42.000Z","categories_index":"MacBook,环境搭建","tags_index":"JetBrains,IDE,MacBook,环境搭建","author_index":"GlassCat"},{"id":"9169b2fd1415ecd8979f95e711397f88","title":"Docker搭建RocketMQ","content":"\n\n\n\n\n\n\n\n\n博客是基于 RocketMQ 4.X 版本，现在又出了 5.X 版本\nRocketMQ 4.X 和 5.X 版本之间有很大的区别，需要谨慎升级 5.X，升级之前要去看看 5.X 官方文档  \n\n5.X 版本主题需要绑定消息类型 链接  \n5.X 版本队列名称改变 链接\n5.X 版本消息具有不可变性 链接\n5.X 版本不再需要生产者组 链接\n5.X 版本消费者组兼容性 链接\n5.X 版本消费者兼容性 链接\n5.X 版本的 SDK 中删除了消息查询的接口\n\n从概念上来看，其实 5.X 版本是更加规范的，SDK 更加轻量化；不过 SDK 删除了消息查询接口十分让人抓狂，消息发送超时了无法回查，只能重试，消费端必须要搞消息去重和幂等\n镜像拉取RocketMQ 镜像\nbashdocker pull apache/rocketmq:4.9.6RocketMQ Dashboard 镜像\nGithub地址\nbashdocker pull apacherocketmq/rocketmq-dashboard:2.0.1创建容器共享网络RocketMQ中有多个服务，需要创建多个容器，创建 docker 网络便于容器间相互通信\nbashdocker network create rocketmq启动 NameServer在本机创建NameServer目录\nbashmkdir -p /Users/oak/docker/rocketmq/namesrv/logs启动 Nameserver\nbashdocker run -d \\\n--name rmqnamesrv \\\n--net rocketmq \\\n-p 9876:9876 \\\n-v /Users/oak/docker/rocketmq/namesrv/logs:/home/rocketmq/logs \\\n-e &quot;JAVA_OPT_EXT=-Xms512M -Xmx512M -Xmn128m&quot; \\\n-e &quot;MAX_POSSIBLE_HEAP=100000000&quot; \\\napache/rocketmq:4.9.6 sh mqnamesrv验证 NameServer 是否启动成功\ntxtdocker logs -f rmqnamesrv看到 ‘The Name Server boot success..’, 表示 NameServer 已成功启动\n启动 Broker在本机创建 Broker 目录\nbashmkdir -p /Users/oak/docker/rocketmq/broker/logs /Users/oak/docker/rocketmq/broker/store /Users/oak/docker/rocketmq/broker/conf再本机 broker 的 conf目录下配置文件 broker.conf\npropertiesbrokerClusterName = DefaultCluster\nbrokerName = broker-a\nbrokerId = 0\ndeleteWhen = 04\nfileReservedTime = 48\nbrokerRole = ASYNC_MASTER\nflushDiskType = ASYNC_FLUSH\n# 这里设置为宿主机的 IP \n# 如果不需要 Dashboard，设置为 127.0.0.1 也行\n# 如果要 Dashboard 就要设置为宿主机 IP，不过宿主机IP变了，这里也要一起改变\nbrokerIP1 = 192.168.1.xxxNameServer 成功启动后，再启动 Broker\nMAX_POSSIBLE_HEAP 参数设置 Broker 服务的最大堆内存，如果太小启动会失败\nbashdocker run -d \\\n--name rmqbroker \\\n--net rocketmq \\\n-p 10912:10912 -p 10911:10911 -p 10909:10909 \\\n-v  /Users/oak/docker/rocketmq/broker/logs:/home/rocketmq/logs \\\n-v  /Users/oak/docker/rocketmq/broker/store:/home/rocketmq/store \\\n-v  /Users/oak/docker/rocketmq/broker/conf/broker.conf:/home/rocketmq/rocketmq-4.9.6/conf/broker.conf \\\n-e &quot;JAVA_OPT_EXT=-Xms1024M -Xmx1024M -Xmn128m&quot; \\\n-e &quot;MAX_POSSIBLE_HEAP=200000000&quot; \\\n-e &quot;NAMESRV_ADDR=rmqnamesrv:9876&quot; \\\napache/rocketmq:4.9.6 sh mqbroker \\\n-c /home/rocketmq/rocketmq-4.9.6/conf/broker.conf验证 Broker 是否启动成功\nbashdocker logs rmqbroker看到 ‘The broker boot success..’, 表示 Broker 已成功启动\n启动 Dashboardbashdocker run -d \\\n--name rmqdashboard \\\n--net rocketmq \\\n-e &quot;JAVA_OPTS=-Drocketmq.namesrv.addr=rmqnamesrv:9876 -Dserver.port=8080 -Dcom.rocketmq.sendMessageWithVIPChannel=false&quot; \\\n-p 18080:8080 \\\n-t apacherocketmq/rocketmq-dashboard:2.0.1验证 Dashboard 是否启动成功，直接访问 Dashboard \nDashboard是支持 4.X 和 5.X 版本的, 可以在界面上切换版本，参考官方使用文档\n\nBroker 配置详解\n 展开查看broker.conf\nproperties#所属集群名字\nbrokerClusterName=DefaultCluster\n\n#broker名字，注意此处不同的配置文件填写的不一样，如果在broker-a.properties使用:broker-a,\n#在broker-b.properties使用:broker-b\nbrokerName=broker-a\n\n#0 表示Master，&gt;0 表示Slave\nbrokerId=0\n\n#nameServer地址，分号分割\n#namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876\nnamesrvAddr=172.168.1.xxx:9876\n\n#启动IP,如果 docker 报 com.alibaba.rocketmq.remoting.exception.RemotingConnectException: connect to &lt;192.168.0.120:10909&gt; failed\n# 解决方式1 加上一句producer.setVipChannelEnabled(false);\n# 解决方式2 brokerIP1 设置宿主机IP，不要使用docker 内部IP\nbrokerIP1=127.0.0.1\n\n#在发送消息时，自动创建服务器不存在的topic，默认创建的队列数\ndefaultTopicQueueNums=4\n\n#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭 ！！！\nautoCreateTopicEnable=true\n\n#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭\nautoCreateSubscriptionGroup=true\n\n#此参数控制是否开启密码\naclEnable=false\n\n#Broker 对外服务的监听端口\nlistenPort=10911\n\n#删除文件时间点，默认凌晨4点\ndeleteWhen=04\n\n#文件保留时间，默认48小时\nfileReservedTime=120\n\n#commitLog每个文件的大小默认1G\nmapedFileSizeCommitLog=1073741824\n\n#ConsumeQueue每个文件默认存30W条，根据业务情况调整\nmapedFileSizeConsumeQueue=300000\n\n#destroyMapedFileIntervalForcibly=120000\n#redeleteHangedFileInterval=120000\n#检测物理文件磁盘空间\ndiskMaxUsedSpaceRatio=88\n#存储路径\n#storePathRootDir=/home/ztztdata/rocketmq-all-4.1.0-incubating/store\n#commitLog 存储路径\n#storePathCommitLog=/home/ztztdata/rocketmq-all-4.1.0-incubating/store/commitlog\n#消费队列存储\n#storePathConsumeQueue=/home/ztztdata/rocketmq-all-4.1.0-incubating/store/consumequeue\n#消息索引存储路径\n#storePathIndex=/home/ztztdata/rocketmq-all-4.1.0-incubating/store/index\n#checkpoint 文件存储路径\n#storeCheckpoint=/home/ztztdata/rocketmq-all-4.1.0-incubating/store/checkpoint\n#abort 文件存储路径\n#abortFile=/home/ztztdata/rocketmq-all-4.1.0-incubating/store/abort\n#限制的消息大小\nmaxMessageSize=65536\n\n#flushCommitLogLeastPages=4\n#flushConsumeQueueLeastPages=2\n#flushCommitLogThoroughInterval=10000\n#flushConsumeQueueThoroughInterval=60000\n\n#Broker 的角色\n#- ASYNC_MASTER 异步复制Master\n#- SYNC_MASTER 同步双写Master\n#- SLAVE\nbrokerRole=ASYNC_MASTER\n\n#刷盘方式\n#- ASYNC_FLUSH 异步刷盘\n#- SYNC_FLUSH 同步刷盘\nflushDiskType=ASYNC_FLUSH\n\n#发消息线程池数量\n#sendMessageThreadPoolNums=128\n#拉消息线程池数量\n#pullMessageThreadPoolNums=128\n\n我的 Broker 没有开启 ACL\n如果 Broker 开启了 ACL ，还要创建 /Users/oak/docker/rocketmq/broker/conf/plain_acl.yml\n通过 Docker 启动的参数要加一行挂载命令\ntxt-v /Users/oak/docker/rocketmq/broker/conf/plain_acl.yml:/home/rocketmq/rocketmq-4.9.6/conf/plain_acl.yml调用方服务的 ip 如果在 globalWhiteRemoteAddresses 白名单中，不会走 acl 鉴权\nyamlglobalWhiteRemoteAddresses:\n- 10.10.103.*\n- 172.168.1.*\n\naccounts:\n- accessKey: RocketMQ\n  secretKey: 12345678\n  whiteRemoteAddress:\n  admin: false\n  defaultTopicPerm: DENY\n  defaultGroupPerm: SUB\n  topicPerms:\n  - topicA=DENY\n  - topicB=PUB|SUB\n  - topicC=SUB\n  groupPerms:\n  # the group should convert to retry topic\n  - groupA=DENY\n  - groupB=PUB|SUB\n  - groupC=SUB\n\n- accessKey: rocketmq2\n  secretKey: 12345678\n  whiteRemoteAddress: 172.168.1.*\n  # if it is admin, it could access all resources\n  admin: true","slug":"Docker搭建RocketMQ","date":"2021-10-04T07:13:11.000Z","categories_index":"Docker,RocketMQ","tags_index":"Docker,RocketMQ","author_index":"GlassCat"},{"id":"132a6bcb91ad26ef458d1f48fbda5347","title":"ES-05 Elasticsearch定制分词器","content":"索引的分析索引分析: 就是把输入的文本块按照一定的策略进行分解, 并建立倒排索引的过程. 在Lucene的架构中, 这个过程由分析器(analyzer)完成.\n1.1 Analyzer的组成\n① Char filter(字符过滤器): 去除HTML标签、把&amp;替换为and等.\n② Tokenizer(分词器): 按照某种规律, 如根据空格、逗号等, 将文本块进行分解.\n③ Token Filter(标记过滤器): 所有被分词器分解的词都将经过 token filters 的处理, 它可以修改词(如小写化处理)、去掉词(根据某一规则去掉无意义的词, 如”a”, “the”, “的”等),  增加词(如同义词 “pater”、”father” 等).\n\n\n\n\n\n\n\n\n\n注意: 人们一般将分析器通称为分词器, 并不是相等的关系, 而是包含的关系.\n1.2 倒排索引的核心原理-normalization建立倒排索引时, 会执行normalization(正常化)操作 —— 将拆分的各个单词进行处理, 以提高搜索时命中关联的文档的概率.\nnormalization的方式有: 时态转换, 单复数转换, 同义词转换, 大小写转换等.\n\n\n\n\n\n\n\n\n\n比如文档中包含His mom likes small dogs:① 在建立索引的时候normalization会对文档进行时态、单复数、同义词等方面的处理;② 然后用户通过近似的mother liked little dog, 也能搜索到相关的文档.\nES的默认分词器(1) ES中的默认分词器: standard tokenizer, 是标准分词器, 它以单词为边界进行分词. 具有如下功能:\n\n\n\n\n\n\n\n\n\n① standard token filter: 去掉无意义的标签, 如&lt;&gt;, &amp;, - 等.② lowercase token filter: 将所有字母转换为小写字母.③ stop token filer (默认被禁用) : 移除停用词, 比如”a”、”the”等.\n(2) 测试默认分词器:\nbash# ES引擎中已有standard分词器, 所以可以不指定index\nGET _analyze\n&#123;\n    &quot;analyzer&quot;: &quot;standard&quot;, \n    &quot;text&quot;: &quot;There-is &amp; a DOG&lt;br/&gt; in house&quot;\n&#125;可以发现, Elasticsearch对text文本进行了分析处理, 结果如下:\njson&#123;\n  &quot;tokens&quot; : [\n    &#123;\n      &quot;token&quot; : &quot;there&quot;,      // 分词\n      &quot;start_offset&quot; : 0,     // 起始偏移量\n      &quot;end_offset&quot; : 5,       // 结束偏移量\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,  // 分词的类型\n      &quot;position&quot; : 0          // 该分词在文本中的位置\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;is&quot;,\n      &quot;start_offset&quot; : 6,\n      &quot;end_offset&quot; : 8,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 1\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;a&quot;,\n      &quot;start_offset&quot; : 11,\n      &quot;end_offset&quot; : 12,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 2\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;dog&quot;,\n      &quot;start_offset&quot; : 13,\n      &quot;end_offset&quot; : 16,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 3\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;br&quot;,\n      &quot;start_offset&quot; : 17,\n      &quot;end_offset&quot; : 19,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 4\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;in&quot;,\n      &quot;start_offset&quot; : 22,\n      &quot;end_offset&quot; : 24,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 5\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;house&quot;,\n      &quot;start_offset&quot; : 25,\n      &quot;end_offset&quot; : 30,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 6\n    &#125;\n  ]\n&#125;修改分词器(1) 创建索引后可以添加新的分词器:\n\n\n\n\n\n\n\n\n\n说明: 必须先关闭索引, 添加完成后, 再及时打开索引进行搜索等操作, 否则将出现错误.\nbash# 关闭索引\nPOST articles/_close\n\n# 启用 English 停用词 token filter\nPUT articles/_settings\n&#123;\n  &quot;analysis&quot;: &#123;\n    &quot;analyzer&quot;: &#123;\n        // 自定义分析器的名字\n      &quot;my_token_filter&quot;: &#123;\n        &quot;type&quot;: &quot;standard&quot;,\n        &quot;stopwords&quot;: &quot;_english_&quot;\n      &#125;\n    &#125;\n  &#125;\n&#125;\n\n# 打开索引\nPOST articles/_open(2) 使用具有停词功能的分词器进行分词:\njson# 指定索引\nGET articles/_analyze      \n&#123;\n   # 指定要使用的分词器\n  &quot;analyzer&quot;: &quot;my_token_filter&quot;,   \n  &quot;text&quot;: &quot;There-is &amp; a DOG&lt;br/&gt; in house&quot;\n&#125;(3) 返回结果减少了停用词there, is, &amp;, a, in等:\njson&#123;\n  &quot;tokens&quot; : [\n    &#123;\n      &quot;token&quot; : &quot;dog&quot;,\n      &quot;start_offset&quot; : 13,\n      &quot;end_offset&quot; : 16,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 3\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;br&quot;,\n      &quot;start_offset&quot; : 17,\n      &quot;end_offset&quot; : 19,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 4\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;house&quot;,\n      &quot;start_offset&quot; : 25,\n      &quot;end_offset&quot; : 30,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 6\n    &#125;\n  ]\n&#125;定制分词器4.1 向索引中添加自定义的分词器\n\n\n\n\n\n\n\n\n同样的, 在添加新的分词器之前, 必须先关闭索引, 添加完成后, 再打开索引进行搜索等操作.\ntxt\n# 关闭索引\nPOST articles/_close\n\nPUT articles/_settings\n&#123;\n  &quot;analysis&quot;: &#123;\n    &quot;char_filter&quot;: &#123;\n      &quot;&amp;_to_and&quot;: &#123;\n        &quot;type&quot;: &quot;mapping&quot;,\n        &quot;mappings&quot;: [\n          &quot;&amp; =&gt; and&quot;\n        ]\n      &#125;\n    &#125;,\n    &quot;filter&quot;: &#123;\n      &quot;my_stopwords&quot;: &#123;\n        &quot;type&quot;: &quot;stop&quot;,\n        &quot;stopwords&quot;: [\n          &quot;the&quot;,\n          &quot;a&quot;\n        ]\n      &#125;\n    &#125;,\n    // 自定义的分析器名称\n    &quot;analyzer&quot;: &#123;\n      &quot;my_analyzer&quot;: &#123;\n        &quot;type&quot;: &quot;custom&quot;,\n        // 跳过HTML标签, 将&amp;符号转换为&quot;and&quot;\n        &quot;char_filter&quot;: [\n          &quot;html_strip&quot;,\n          &quot;&amp;_to_and&quot;\n        ],\n        &quot;tokenizer&quot;: &quot;standard&quot;,\n        // 转换为小写\n        &quot;filter&quot;: [\n          &quot;lowercase&quot;,\n          &quot;my_stopwords&quot;\n        ]\n      &#125;\n    &#125;\n  &#125;\n&#125;\n\n# 打开索引\nPOST articles/_open4.2 测试自定义分析器jsonGET articles/_analyze      \n&#123;\n    &quot;analyzer&quot;: &quot;my_analyzer&quot;,   \n    &quot;text&quot;: &quot;There-is &amp; a DOG&lt;br/&gt; in house&quot;\n&#125;可以发现, 返回的分析结果中已经对大写单词、HTML标签, 以及”&amp;”做了处理.\njson&#123;\n  &quot;tokens&quot; : [\n    &#123;\n      &quot;token&quot; : &quot;there&quot;,\n      &quot;start_offset&quot; : 0,\n      &quot;end_offset&quot; : 5,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 0\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;is&quot;,\n      &quot;start_offset&quot; : 6,\n      &quot;end_offset&quot; : 8,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 1\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;and&quot;,\n      &quot;start_offset&quot; : 9,\n      &quot;end_offset&quot; : 10,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 2\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;dog&quot;,\n      &quot;start_offset&quot; : 13,\n      &quot;end_offset&quot; : 16,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 4\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;in&quot;,\n      &quot;start_offset&quot; : 22,\n      &quot;end_offset&quot; : 24,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 5\n    &#125;,\n    &#123;\n      &quot;token&quot; : &quot;house&quot;,\n      &quot;start_offset&quot; : 25,\n      &quot;end_offset&quot; : 30,\n      &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;,\n      &quot;position&quot; : 6\n    &#125;\n  ]\n&#125;4.3 向映射中添加自定义的分词器jsonPUT articles/_mapping?include_type_name=false\n&#123;\n  &quot;properties&quot;: &#123;\n    &quot;desc&quot;: &#123;\n      &quot;type&quot;: &quot;text&quot;,\n      &quot;analyzer&quot;: &quot;my_analyzer&quot;\n    &#125;\n  &#125;\n&#125;此时查看mapping信息:\njsonGET articles?include_type_name=false可以看到自定义的分析器已经配置到 articles 上了:\njson&#123;\n  &quot;properties&quot; : &#123;\n    &quot;author&quot; : &#123;\n      &quot;type&quot; : &quot;keyword&quot;\n    &#125;,\n    &quot;content&quot; : &#123;\n      &quot;type&quot; : &quot;text&quot;\n    &#125;,\n    &quot;desc&quot; : &#123;\n      &quot;type&quot; : &quot;text&quot;,\n      &quot;analyzer&quot; : &quot;my_analyzer&quot;\n    &#125;,\n    &quot;title&quot; : &#123;\n      &quot;type&quot; : &quot;text&quot;\n    &#125;,\n    &quot;words&quot; : &#123;\n      &quot;type&quot; : &quot;long&quot;\n    &#125;\n  &#125;\n&#125;","slug":"ES-05-ElasticSearch定制分词器","date":"2018-09-15T06:26:09.000Z","categories_index":"ElasticSearch系列","tags_index":"ElasticSearch","author_index":"GlassCat"},{"id":"e1de3cc98b0a7233f053d10bd004aae2","title":"ES-04 Elasticsearch索引管理","content":"\n\n\n\n\n\n\n\n\nElasticSearch 系列文章基于 6.7.1 版本，请知悉。\nElasticsearch 中的 index 相当于 MySQL 中的 DataBase.\n官方文档 索引管理\n创建索引5.X版本的创建\n展开查看\n(1) 创建语法:\njsonPUT index\n&#123;\n    &quot;settings&quot;: &#123; ... some settings ... &#125;,\n    &quot;mappings&quot;: &#123;\n        &quot;type1&quot;: &#123; ... some mappings ... &#125;,\n        &quot;type2&quot;: &#123; ... some mappings ... &#125;,\n        ...\n    &#125;\n&#125;如果不指定 settings 和 mappings, 直接插入数据时, ES会根据要插入数据的类型, 自动创建相关配置 —— 功能强大, 但扩展性不够, 后续若有其他原因需要修改mappings, 会很麻烦.\n(2) 创建示例:\n在名为 blog 的 index 下创建了两个 type [ articles , comments ]\njsonPUT blog\n&#123;\n    &quot;settings&quot;: &#123;\n        &quot;number_of_shards&quot;: 1,\n        &quot;number_of_replicas&quot;: 0\n    &#125;,\n    &quot;mappings&quot;: &#123;\n        &quot;articles&quot;: &#123;\n            &quot;properties&quot;: &#123;\n                &quot;title&quot;: &#123;\n                    &quot;type&quot;: &quot;text&quot;\n                &#125;,\n                &quot;author&quot;: &#123;\n                    &quot;type&quot;: &quot;text&quot;\n                &#125;\n            &#125;\n        &#125;,\n        &quot;comments&quot;: &#123;\n            &quot;properties&quot;: &#123;\n                &quot;username&quot;: &#123;\n                    &quot;type&quot;: &quot;text&quot;\n                &#125;,\n                &quot;content&quot;: &#123;\n                    &quot;type&quot;: &quot;text&quot;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;\n\n6.X及版本的创建(1) 创建语法:\njsonPUT index?include_type_name=false\n&#123;\n    &quot;settings&quot;: &#123; ... settings ... &#125;,\n    &quot;mappings&quot;: &#123; ... mappings ... &#125;\n&#125;(2) 创建示例:\n在名为 articles 的 index , 设置一个分片 没有副本\njsonPUT articles?include_type_name=false\n&#123;\n  &quot;settings&quot;: &#123;\n    &quot;number_of_shards&quot;: 1,\n    &quot;number_of_replicas&quot;: 0\n  &#125;,\n  &quot;mappings&quot;: &#123;\n    &quot;properties&quot;: &#123;\n      &quot;title&quot;: &#123;\n        &quot;type&quot;: &quot;text&quot;\n      &#125;,\n      &quot;author&quot;: &#123;\n        &quot;type&quot;: &quot;keyword&quot;\n      &#125;,\n      &quot;content&quot;: &#123;\n        &quot;type&quot;: &quot;text&quot;\n      &#125;,\n      &quot;words&quot;: &#123;\n        &quot;type&quot;: &quot;long&quot;\n      &#125;\n    &#125;\n  &#125;\n&#125;字段类型有很多可以查看 官网文档的字段数据类型\n(3) 创建结果:\njson&#123;\n  &quot;acknowledged&quot; : true,\n  &quot;shards_acknowledged&quot; : true,\n  &quot;index&quot; : &quot;articles&quot;\n&#125;查看index(1) 查看示例:\nbash# 查看索引\nGET articles?include_type_name=true\n\nGET *?include_type_name=true\n\nGET _all?include_type_name=true\n\nGET art*?include_type_name=true\n\nGET *art*?include_type_name=true(2) 查看的结果\njson&#123;\n  &quot;articles&quot; : &#123;\n    &quot;aliases&quot; : &#123; &#125;,\n    &quot;mappings&quot; : &#123;\n      &quot;_doc&quot; : &#123;\n        &quot;properties&quot; : &#123;\n          &quot;author&quot; : &#123;\n            &quot;type&quot; : &quot;keyword&quot;\n          &#125;,\n          &quot;content&quot; : &#123;\n            &quot;type&quot; : &quot;text&quot;\n          &#125;,\n          &quot;title&quot; : &#123;\n            &quot;type&quot; : &quot;text&quot;\n          &#125;,\n          &quot;words&quot; : &#123;\n            &quot;type&quot; : &quot;long&quot;\n          &#125;\n        &#125;\n      &#125;\n    &#125;,\n    &quot;settings&quot; : &#123;\n      &quot;index&quot; : &#123;\n        &quot;creation_date&quot; : &quot;1749143192409&quot;,\n        &quot;number_of_shards&quot; : &quot;1&quot;,\n        &quot;number_of_replicas&quot; : &quot;0&quot;,\n        &quot;uuid&quot; : &quot;BLh_-V54TzOvs-IruB3kCw&quot;,\n        &quot;version&quot; : &#123;\n          &quot;created&quot; : &quot;6070199&quot;\n        &#125;,\n        &quot;provided_name&quot; : &quot;articles&quot;\n      &#125;\n    &#125;\n  &#125;\n&#125;修改index修改索引的示例:\njsonPUT articles/_settings\n&#123;\n    &quot;number_of_replicas&quot;: 1\t\n&#125;说明: Elasticsearch中的分片数(number_of_shards)只能在创建索引时设置, 无论是否添加过数据, 都不支持修改.\n一般来说，现有字段的映射无法更新。此规则有一些例外情况。例如：\n\n\n\n\n\n\n\n\n\n 可以向对象数据类型字段添加新 properties\n可以向现有字段添加新的多字段\nignore_above 参数可以被更新\n举个栗子：\n先创建一个索引 my_index\njsonPUT my_index?include_type_name=false\n&#123;\n  &quot;mappings&quot;: &#123;\n    &quot;properties&quot;: &#123;\n      &quot;name&quot;: &#123;\n        &quot;properties&quot;: &#123;\n          &quot;first&quot;: &#123;\n            &quot;type&quot;: &quot;text&quot;\n          &#125;\n        &#125;\n      &#125;,\n      &quot;user_id&quot;: &#123;\n        &quot;type&quot;: &quot;keyword&quot;\n      &#125;\n    &#125;\n  &#125;\n&#125;修改索引：\n① 在 name 对象字段下添加一个 last 字段 \n② 将 ignore_above 的默认值 0 重新设置\njsonPUT my_index/_mapping?include_type_name=false\n&#123;\n  &quot;properties&quot;: &#123;\n    &quot;name&quot;: &#123;\n      &quot;properties&quot;: &#123;\n        &quot;last&quot;: &#123; \n          &quot;type&quot;: &quot;text&quot;\n        &#125;\n      &#125;\n    &#125;,\n    &quot;user_id&quot;: &#123;\n      &quot;type&quot;: &quot;keyword&quot;,\n      &quot;ignore_above&quot;: 100 \n    &#125;\n  &#125;\n&#125;删除index\n\n\n\n\n\n\n\n\n删除索引需要指明索引名称、别名或通配符.Elasticsearch支持同时删除多个索引, 或使用_all或通配符*删除全部索引.\n删除示例:\nbashDELETE articles        // 删除指定索引\nDELETE index1,index2  // 删除多个索引\nDELETE index_*        // 按通配符删除以&#39;index_&#39;开头的索引\nDELETE _all           // 删除全部索引为避免_all操作误删除全部索引, 可在配置文件elasticsearch.yml中作如下配置:\nyaml# 要求操作索引时必须指定索引的名称\naction.destructive_requires_name: true打开&#x2F;关闭index(1) 操作说明:\n\n\n\n\n\n\n\n\n\n① 可以打开一个已经打开&#x2F;关闭的索引, 以最后一次操作为准;② 可以关闭一个已经关闭&#x2F;打开的索引, 以最后一次操作为准;③ 关闭的索引只能查看index的配置信息, 不能对内部的索引数据进行读写操作.\n(2) 操作示例:\njson// 可以使用_all打开或关闭全部索引, 也可使用通配符(*)配合操作\nPOST articles/_close\nPOST articles/_open说明事项:\n\n\n\n\n\n\n\n\n\n① 使用_all或通配符操作索引, 都会受到配置文件中action.destructive_requires_name=true的限制.② 关闭的索引会继续占用磁盘空间, 却又不能使用 —— 造成磁盘空间的浪费.③ 可以在配置文件中禁止使用关闭索引的功能: cluster.indices.close.enable=false, 默认为true(开启).\n(3) 关闭后查询：\ntxtGET articles/_search\n&#123;\n  &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;\n&#125;报错\njson&#123;\n  &quot;error&quot;: &#123;\n    &quot;root_cause&quot;: [\n      &#123;\n          // 索引关闭异常\n        &quot;type&quot;: &quot;index_closed_exception&quot;,\n        &quot;reason&quot;: &quot;closed&quot;,\n        &quot;index_uuid&quot;: &quot;BLh_-V54TzOvs-IruB3kCw&quot;,\n        &quot;index&quot;: &quot;articles&quot;\n      &#125;\n    ],\n    &quot;type&quot;: &quot;index_closed_exception&quot;,\n    &quot;reason&quot;: &quot;closed&quot;,\n    &quot;index_uuid&quot;: &quot;BLh_-V54TzOvs-IruB3kCw&quot;,\n    &quot;index&quot;: &quot;articles&quot;\n  &#125;,\n  &quot;status&quot;: 400\n&#125;","slug":"ES-04-ElasticSearch索引管理","date":"2018-09-15T05:26:09.000Z","categories_index":"ElasticSearch系列","tags_index":"ElasticSearch","author_index":"GlassCat"},{"id":"73ef3b093e20c3ad31e94ce3ba755d05","title":"ES-03 Elasticsearch的简单查询用法","content":"Query String Search (查询字符串检索)这种方法通过 HTTP 请求的 Query String 携带查询参数, 因此得名.\n适用于临时性的查询请求, 比如在终端检查基础信息:\nbash# 检索name中包含Java的文档, 并按价格降序排序: \ncurl -XGET &#39;http://localhost:9200/blog/_doc/_search?q=name:Kibana&amp;sort=date:desc&#39; 生产环境中很少使用, 因为请求参数都封装到 Query String 中,  难以构建复杂的查询.\n(1) 查询全部文章:\n直接在浏览器的地址栏内输入:\nbashhttp://localhost:9200/blog/_doc/_search?q=name:Kibana(2) 查询的结果:\n\n展开查看\njson&#123;\n  &quot;took&quot;: 7,\n  &quot;timed_out&quot;: false,\n  &quot;_shards&quot;: &#123;\n    &quot;total&quot;: 5,\n    &quot;successful&quot;: 5,\n    &quot;skipped&quot;: 0,\n    &quot;failed&quot;: 0\n  &#125;,\n  &quot;hits&quot;: &#123;\n    &quot;total&quot;: 1,\n    &quot;max_score&quot;: 0.2876821,\n    &quot;hits&quot;: [\n      &#123;\n        &quot;_index&quot;: &quot;blog&quot;,\n        &quot;_type&quot;: &quot;_doc&quot;,\n        &quot;_id&quot;: &quot;1&quot;,\n        &quot;_score&quot;: 0.2876821,\n        &quot;_source&quot;: &#123;\n          &quot;name&quot;: &quot;Kibana使用方法&quot;,\n          &quot;author&quot;: &quot;GlassCat&quot;,\n          &quot;category&quot;: &quot;ElasticSearch系列&quot;,\n          &quot;desc&quot;: &quot;ES学习必读经典,殿堂级著作！&quot;,\n          &quot;words&quot;: 1932,\n          &quot;publisher&quot;: &quot;XX工业出版社&quot;,\n          &quot;date&quot;: &quot;2018-10-14&quot;,\n          &quot;tags&quot;: [\n            &quot;ElasticSearch&quot;,\n            &quot;Kibana&quot;\n          ]\n        &#125;\n      &#125;\n    ]\n  &#125;\n&#125;\n\n(3) 查询结果中的各个参数的含义:\n\n\n\n\n\n\n\n\n\n① took: 此次检索耗费的时间, 单位是毫秒;\n② timed_out: 是否超出规定的检索时间, 这里没有设置, 后续会讲解此参数;\n③ _shards: 被查询的index被分散成多个分片, 所以搜索请求会分发到所有的primary shard(或primary shard对应的某个replica shard)上, 这里显示各个分片是否查询成功的信息;\n④ hits: 命中的文档情况, 有如下参数:\n\ntotal: 符合条件的文档总数, 即hit(命中)数;max_score: Lucene底层对检索到的文档的相关度的评分, 相关度越高, 说明越匹配, score的值也就越高.hits: 命中的所有document的详细数据.\n\nQuery DSL(ES特定语法检索)\n\n\n\n\n\n\n\n\nDSL: Domain Specified Language, 特定领域的语言, 一般需要Kibana等工具配合操作.\n这种方式把查询参数构建成JSON格式的数据, 并封装到HTTP请求的Request Body(请求体)中, 可以构建各类复杂的查询语法, 功能要比Query String Search强大很多.\n(1) 查询全部文章:\njsonGET blog/_search\n&#123;\n    &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;\n&#125;(2) 查询 name 中包含 ES 的文章, 并按 words 降序排序:\njsonGET blog/_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;match&quot;: &#123;\n            &quot;name&quot;: &quot;ES&quot;\n        &#125;\n    &#125;,\n    &quot;sort&quot;: [\n        &#123; &quot;words&quot;: &quot;desc&quot; &#125;\n    ]\n&#125;(3) 分页查询 - 每页显示1条, 显示第2页:\njsonGET blog/_search\n&#123;\n    &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;,\n    &quot;from&quot;: 1,\n    &quot;size&quot;: 1\n&#125;(4) 只查询 name 和 author 字段:\njsonGET blog/_search\n&#123;\n    &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;,\n    &quot;_source&quot;: [&quot;name&quot;, &quot;author&quot;]\n&#125;Query Filter(过滤检索)\n\n\n\n\n\n\n\n\n过滤查询, 比如: 查询 name 中包含 ES , 且 words 不大于 1000 的文章:\njsonGET blog/_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;bool&quot;: &#123;\n              &quot;must&quot;: &#123;\n                &quot;match&quot;: &#123;&quot;name&quot;: &quot;ES&quot;&#125;\t\n            &#125;,\n            &quot;filter&quot;: &#123;\n                &quot;range&quot;: &#123; \n                    &quot;words&quot;: &#123;&quot;lte&quot;: 1000&#125;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125;Full Text Search(全文检索)(1) 查询 desc 中包含”使用方法”的文档, 只显示 name 和 desc 的值:\njsonGET blog/_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;match&quot;: &#123;&quot;desc&quot;: &quot;使用方法&quot;&#125;\n    &#125;,\n    &quot;_source&quot;: [&quot;name&quot;, &quot;desc&quot;]\n&#125;(2) 查询结果中有2条数据符合要求:\njson&#123;\n  &quot;took&quot; : 2,\n  &quot;timed_out&quot; : false,\n  &quot;_shards&quot; : &#123;\n    &quot;total&quot; : 5,\n    &quot;successful&quot; : 5,\n    &quot;skipped&quot; : 0,\n    &quot;failed&quot; : 0\n  &#125;,\n  &quot;hits&quot; : &#123;\n    &quot;total&quot; : 2,\n    &quot;max_score&quot; : 1.1507283,\n    &quot;hits&quot; : [\n      &#123;\n        &quot;_index&quot; : &quot;blog&quot;,\n        &quot;_type&quot; : &quot;_doc&quot;,\n        &quot;_id&quot; : &quot;2&quot;,\n        &quot;_score&quot; : 1.1507283,\n        &quot;_source&quot; : &#123;\n          &quot;name&quot; : &quot;ES使用方法&quot;,\n          &quot;desc&quot; : &quot;ES使用方法&quot;\n        &#125;\n      &#125;,\n      &#123;\n        &quot;_index&quot; : &quot;blog&quot;,\n        &quot;_type&quot; : &quot;_doc&quot;,\n        &quot;_id&quot; : &quot;3&quot;,\n        &quot;_score&quot; : 0.5753642,\n        &quot;_source&quot; : &#123;\n          &quot;name&quot; : &quot;ES高阶用法&quot;,\n          &quot;desc&quot; : &quot;ES高阶用法&quot;\n        &#125;\n      &#125;\n    ]\n  &#125;\n&#125;(3) 全文检索的过程 —— 对查询结果的说明:\n\n\n\n\n\n\n\n\n\nElasticsearch会对字段”desc”的内容进行分词, 并建立倒排索引.\n也就是说, 这里会把 “使用方法” 分词为 使、用、方、法，4个词, 检索时将匹配 desc 中含有4个词中任意一个分词的文档.\n—— 对于中文分词, 可以通过IK分词器, 把”使用方法”分解为”使用”、”方法” 2个词, 后面的文章会说到\nPhrase Search(短语检索)\n\n\n\n\n\n\n\n\nFull Text Search会对检索文本作分词处理, 然后从倒排索引中作匹配查询, 如果一个文档的对应field中存在任意一个分解后的词, 那么这个文档就算匹配检索条件.\nPhrase Search不会对检索串进行分词处理, 只有一个文档的对应field中包含与检索文本完全一致的内容, 该文档才算匹配检索条件, 也才能作为结果返回 —— 可以理解为全文检索场景下的部分精确匹配.\n(1) 精确查询desc中包含”Java图书”的文档:\njsonGET blog/_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;match_phrase&quot;: &#123;\n            &quot;desc&quot;: &quot;使用方法&quot;\n        &#125;\n    &#125;,\n    &quot;_source&quot;: [&quot;name&quot;, &quot;desc&quot;]\n&#125;(2) 查询结果只有一条数据符合要求了:\njson&#123;\n  &quot;took&quot; : 18,\n  &quot;timed_out&quot; : false,\n  &quot;_shards&quot; : &#123;\n    &quot;total&quot; : 5,\n    &quot;successful&quot; : 5,\n    &quot;skipped&quot; : 0,\n    &quot;failed&quot; : 0\n  &#125;,\n  &quot;hits&quot; : &#123;\n    &quot;total&quot; : 1,\n    &quot;max_score&quot; : 1.1507283,\n    &quot;hits&quot; : [\n      &#123;\n        &quot;_index&quot; : &quot;blog&quot;,\n        &quot;_type&quot; : &quot;_doc&quot;,\n        &quot;_id&quot; : &quot;2&quot;,\n        &quot;_score&quot; : 1.1507283,\n        &quot;_source&quot; : &#123;\n          &quot;name&quot; : &quot;ES使用方法&quot;,\n          &quot;desc&quot; : &quot;ES使用方法&quot;\n        &#125;\n      &#125;\n    ]\n  &#125;\n&#125;Highlight Search(高亮检索)(1) 分页查询 desc 中包含”使用方法”的文档, 页大小为1, 显示第1页, 并对搜索条件高亮处理:\njsonGET blog/_search\n&#123;\n    &quot;query&quot;: &#123;\n        &quot;match&quot;: &#123;&quot;desc&quot;: &quot;使用方法&quot;&#125;\n    &#125;,\n    &quot;from&quot;: 0,\n    &quot;size&quot;: 1,\n    &quot;highlight&quot;: &#123;\n        &quot;fields&quot;: &#123;&quot;desc&quot;: &#123;&#125;&#125;\n    &#125;,\n    &quot;_source&quot;: [&quot;name&quot;, &quot;desc&quot;]\n&#125;(2) 查询结果:\njson&#123;\n  &quot;took&quot; : 14,\n  &quot;timed_out&quot; : false,\n  &quot;_shards&quot; : &#123;\n    &quot;total&quot; : 5,\n    &quot;successful&quot; : 5,\n    &quot;skipped&quot; : 0,\n    &quot;failed&quot; : 0\n  &#125;,\n  &quot;hits&quot; : &#123;\n    &quot;total&quot; : 2,\n    &quot;max_score&quot; : 1.1507283,\n    &quot;hits&quot; : [\n      &#123;\n        &quot;_index&quot; : &quot;blog&quot;,\n        &quot;_type&quot; : &quot;_doc&quot;,\n        &quot;_id&quot; : &quot;2&quot;,\n        &quot;_score&quot; : 1.1507283,\n        &quot;_source&quot; : &#123;\n          &quot;name&quot; : &quot;ES使用方法&quot;,\n          &quot;desc&quot; : &quot;ES使用方法&quot;\n        &#125;,\n        &quot;highlight&quot; : &#123;\n          &quot;desc&quot; : [\n            &quot;ES&lt;em&gt;使&lt;/em&gt;&lt;em&gt;用&lt;/em&gt;&lt;em&gt;方&lt;/em&gt;&lt;em&gt;法&lt;/em&gt;&quot;\n          ]\n        &#125;\n      &#125;\n    ]\n  &#125;\n&#125;\n从上述结果的&lt;em&gt;使&lt;/em&gt;&lt;em&gt;用&lt;/em&gt;&lt;em&gt;方&lt;/em&gt;&lt;em&gt;法&lt;/em&gt;也可以看出, ES底层对 desc 字段的值”使用方法”进行了分词处理\n本文的六种查询方法, 只是一个简单的入门, 详细使用方法会在后续的学习中逐一演示.\n","slug":"ES-03-ElasticSearch查询文档","date":"2018-09-14T04:56:09.000Z","categories_index":"ElasticSearch系列","tags_index":"ElasticSearch","author_index":"GlassCat"},{"id":"4775e6c7c44b69c5a8eee7dc43222d3a","title":"ES-02 Kibana的使用方法","content":"安装方法参考我之前的 Docker安装Kibana+ElasticSearch\nKibana基础功能前往 Kibana界面 的 Dev Tools 界面, 执行如下命令:\nbashGET _cluster/health得到如下关于集群健康状况的JSON响应:\njson&#123;\n  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,\n  &quot;status&quot; : &quot;red&quot;,\n  &quot;timed_out&quot; : false,\n  // 集群中的节点数\n  &quot;number_of_nodes&quot; : 1, \n  // 存储数据的节点数\n  &quot;number_of_data_nodes&quot; : 1,\n  // 活跃的primary shard数\n  &quot;active_primary_shards&quot; : 14, \n  // 活跃的shard数, 包括primary shard和replica shard\n  &quot;active_shards&quot; : 14, \n  // 当前正在从一个节点迁往其他节点的分片的数量, 通常是0. 当ES发现集群不太均衡(如添加或下线一个节点)时, 该值会上涨\n  &quot;relocating_shards&quot; : 0, \n  // 刚创建的分片个数, 创建第一个索引时、节点重启时, 会短暂处于此状态(不应长期停留此状态)\n  &quot;initializing_shards&quot; : 0,\n  // 在集群中存在, 却又不能找到 -- 即未分配的副本\n  &quot;unassigned_shards&quot; : 2,\n  &quot;delayed_unassigned_shards&quot; : 0,\n  &quot;number_of_pending_tasks&quot; : 0,\n  &quot;number_of_in_flight_fetch&quot; : 0,\n  &quot;task_max_waiting_in_queue_millis&quot; : 0,\n  &quot;active_shards_percent_as_number&quot; : 87.5\n&#125;\n集群的状态status\n\n\n\n\n\n\n\n\n① green: 所有primary shard和replica shard都已成功分配, 集群是100%可用的;\n② yellow: 所有primary shard都已成功分配, 但至少有一个replica shard缺失. 此时集群所有功能都正常使用, 数据不会丢失, 搜索结果依然完整, 但集群的可用性减弱. —— 需要及时处理的警告.\n③ red: 至少有一个 primary shard (以及它的全部副本分片)缺失 —— 部分数据不能使用, 搜索只能返回部分数据, 而分配到这个分配上的写入请求会返回一个异常. 此时虽然可以运行部分功能, 但为了索引数据的完整性, 需要尽快修复集群.\n集群状态为什么是yellow?\n\n\n\n\n\n\n\n\n\n① 当前只有一个Elasticsearch节点, 而且此时ES中只有一个Kibana内建的索引数据.② ES为每个index默认分配5个primary shard和5个replica shard, 为了保证高可用, 它还要求primary shard和replica shard不能在同一个node上.③ 当前服务中, Kibana内建的index是1个primary shard和1个replica shard, 由于只有1个node, 所以只有primary shard被分配和启动了, 而replica shard没有被成功分配(没有其他node可用).\n集群中的节点数\n\n\n\n\n\n\n\n\n使用GET _cat/nodes?v 查看当前节点数:\nbaship            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name\n172.16.22.133           62          98   2    0.98    1.25     1.39 mdi       *      1UlY804\n未分配的分片\nunassigned_shards: 已经在集群状态中存在、但在集群里又找不到的分片, 来源通常是未分配的副本. 比如: 一个有 5 分片和 1 副本的索引, 在单节点集群上就会有 5 个未分配副本分片.如果集群状态是red, 也会长期存在未分配分片(因为缺少主分片).\n索引 Index 管理创建索引bash# 创建索引API: \nPUT test_index?pretty\n# 响应信息如下:\n#! Deprecation: the default number of shards will change from [5] to [1] in 7.0.0; if you wish to continue using the default of [5] shards, you must manage this on the create index request or with an index template\n# 过时说明:在创建索引时, Elasticsearch提出过时警告: 从7.0.0版本开始, 默认的Shard个数将从[5]变为[1].如果要继续使用默认的[5]个分片(Shard), 就需要在创建Index时指定, 或者通过索引模板创建Index.\n&#123;\n  &quot;acknowledged&quot; : true,\n  &quot;shards_acknowledged&quot; : true,\n  &quot;index&quot; : &quot;test_index&quot;\n&#125;查询索引bash# 查询所有的索引\nGET /_cat/indices?v\n\nhealth status index                           uuid                   pri rep docs.count docs.deleted store.size pri.store.size\n\nyellow open   test_index                      JYXAg2VfQTC9NnLFdflzFA   5   1          0            0       460b           460b删除索引bashDELETE test_index?pretty\n\n# 响应信息如下: \n&#123;\n  &quot;acknowledged&quot;: true\n&#125;文档 Doc 管理ES是一款面向文档的数据搜索、分析引擎. Document结构说明:\n\n(1) 基于面向对象的开发思想, 应用系统中的数据结构都是很复杂的: 对象中嵌套对象, 如CRM系统中的客户对象中, 还会嵌入客户相关的企业对象.\n(2) 对象数据存储到数据库中, 需要分解, 将嵌套对象分解为扁平的多张表数据, 每次操作时需要还原回对象格式, 过程繁琐.\n(3) ES存储的是JSON格式的文档, 基于此, ES可以提供复杂的索引, 全文检索, 分析聚合等功能.\n(4) document格式示例:\njson&#123;\n    &quot;id&quot;: &quot;1000&quot;,\n    &quot;name&quot;: &quot;杰尼龟&quot;,\n    &quot;sex&quot;: &quot;男&quot;,\n    &quot;age&quot;: 25, \n    &quot;phone&quot;: 13312341234, \n    &quot;email&quot;: &quot;12345@qq.com&quot;,\n    &quot;company&quot;: &#123;\n        &quot;name&quot;: &quot;Alibaba&quot;,\n        &quot;location&quot;: &quot;杭州&quot;\n    &#125;,\n    &quot;join_date&quot;: &quot;2018/01/01&quot;\n&#125;\n添加文档添加API语法\njsonPUT index/_doc/id\n&#123;\n  &quot;key1&quot;:&quot;value1&quot;,\n  &quot;key2&quot;:&quot;value2&quot;\n&#125;添加 id &#x3D; 1 的文档\nbashPUT blog/_doc/1?pretty\n&#123;\n    &quot;name&quot;: &quot;Kibana使用方法&quot;,\n    &quot;author&quot;: &quot;GlassCat&quot;,\n    &quot;category&quot;: &quot;ElasticSearch系列&quot;,\n    &quot;desc&quot;:  &quot;ES学习必读经典,殿堂级著作！&quot;,\n    &quot;words&quot;:  1024,\n    &quot;publisher&quot;: &quot;GlassCat&#39;s blog&quot;,\n    &quot;date&quot;: &quot;2018-09-14&quot;,\n    &quot;tags&quot;: [ &quot;ElasticSearch&quot;, &quot;搜索引擎&quot; ]\n&#125;响应\ntxt&#123;\n  &quot;_index&quot; : &quot;blog&quot;,\n  &quot;_type&quot; : &quot;_doc&quot;,\n  &quot;_id&quot; : &quot;1&quot;,\n  &quot;_version&quot; : 1,\n  &quot;result&quot; : &quot;created&quot;,\n  &quot;_shards&quot; : &#123;\n    &quot;total&quot; : 2,\n    &quot;successful&quot; : 1,\n    &quot;failed&quot; : 0\n  &#125;,\n  &quot;_seq_no&quot; : 0,\n  &quot;_primary_term&quot; : 1\n&#125;\n查询文档查询 id &#x3D; 1 的文档\nbashGET blog/_doc/1?pretty响应\njson&#123;\n  &quot;_index&quot; : &quot;blog&quot;,\n  &quot;_type&quot; : &quot;_doc&quot;,\n  &quot;_id&quot; : &quot;1&quot;,\n  &quot;_version&quot; : 1,\n  &quot;_seq_no&quot; : 0,\n  &quot;_primary_term&quot; : 1,\n  &quot;found&quot; : true,\n  &quot;_source&quot; : &#123;\n    &quot;name&quot; : &quot;Kibana使用方法&quot;,\n    &quot;author&quot; : &quot;GlassCat&quot;,\n    &quot;category&quot; : &quot;ElasticSearch系列&quot;,\n    &quot;desc&quot; : &quot;ES学习必读经典,殿堂级著作！&quot;,\n    &quot;words&quot; : 1024,\n    &quot;publisher&quot; : &quot;XX工业出版社&quot;,\n    &quot;date&quot; : &quot;2018-09-14&quot;,\n    &quot;tags&quot; : [\n      &quot;ElasticSearch&quot;,\n      &quot;搜索引擎&quot;\n    ]\n  &#125;\n&#125;更新文档部分更新更新博客的作者\nbashPOST blog/_doc/1/_update?pretty\n&#123;\n  &quot;doc&quot;: &#123;\n    &quot;author&quot;: &quot;Jane Doe&quot;\n  &#125;\n&#125;txt&#123;\n  &quot;_index&quot; : &quot;blog&quot;,\n  &quot;_type&quot; : &quot;_doc&quot;,\n  &quot;_id&quot; : &quot;1&quot;,\n  &quot;_version&quot; : 2,\n  &quot;result&quot; : &quot;updated&quot;,\n  &quot;_shards&quot; : &#123;\n    &quot;total&quot; : 2,\n    &quot;successful&quot; : 1,\n    &quot;failed&quot; : 0\n  &#125;,\n  &quot;_seq_no&quot; : 1,\n  &quot;_primary_term&quot; : 1\n&#125;\n更新文档的字数，通过脚本的方式\nbash# 脚本更新文档\nPOST blog/_doc/1/_update?pretty\n&#123;\n  &quot;script&quot; : &quot;ctx._source.words += 5&quot;\n&#125;文档替换替换API 与 添加API 相同, 只不过文档 id 要存在, 此时系统将判定为修改操作。\n—— 无论文档数据是否存在修改, 对同一个 id 的文档执行1次以上的PUT操作, 都是修改操作.\nbashPUT blog/_doc/1?pretty\n&#123;\n    &quot;name&quot;: &quot;Kibana使用方法【修订版】&quot;,\n    &quot;author&quot;: &quot;GlassCat&quot;,\n    &quot;category&quot;: &quot;ElasticSearch系列&quot;,\n    &quot;desc&quot;:  &quot;ES学习必读经典,殿堂级著作！&quot;,\n    &quot;words&quot;:  1932,\n    &quot;publisher&quot;: &quot;XX工业出版社&quot;,\n    &quot;date&quot;: &quot;2018-10-14&quot;,\n    &quot;tags&quot;: [ &quot;ElasticSearch&quot;, &quot;搜索引擎&quot; ]\n&#125;json&#123;\n  &quot;_index&quot; : &quot;blog&quot;,\n  &quot;_type&quot; : &quot;_doc&quot;,\n  &quot;_id&quot; : &quot;1&quot;,\n  &quot;_version&quot; : 5,\n  // 创建的时候是created, 替换更新是updated\n  &quot;result&quot; : &quot;updated&quot;,\n  &quot;_shards&quot; : &#123;\n    &quot;total&quot; : 2,\n    &quot;successful&quot; : 1,\n    &quot;failed&quot; : 0\n  &#125;,\n  &quot;_seq_no&quot; : 4,\n  &quot;_primary_term&quot; : 1\n&#125;\n删除文档删除id&#x3D;1的文档\nbashDELETE blog/_doc/1txt&#123;\n  &quot;_index&quot; : &quot;blog&quot;,\n  &quot;_type&quot; : &quot;_doc&quot;,\n  &quot;_id&quot; : &quot;1&quot;,\n  &quot;_version&quot; : 7,\n  &quot;result&quot; : &quot;deleted&quot;,\n  &quot;_shards&quot; : &#123;\n    &quot;total&quot; : 2,\n    &quot;successful&quot; : 1,\n    &quot;failed&quot; : 0\n  &#125;,\n  &quot;_seq_no&quot; : 6,\n  &quot;_primary_term&quot; : 1\n&#125;Cat APIbash\n# 查看集群的健康状态\nGET _cat/health?v\n\n# 查看集群的nodes\nGET _cat/nodes?v\n\n# 查看集群中所有的索引\nGET /_cat/indices?v","slug":"ES-02-Kibana的使用方法","date":"2018-09-14T04:26:09.000Z","categories_index":"ElasticSearch系列","tags_index":"ElasticSearch,Kibana","author_index":"GlassCat"},{"id":"4cd15f1f7166f2cddbe2d554199d5739","title":"ES-01 ElasticSearch基础概念","content":"Elasticsearch简介\n\n\n\n\n\n\n\n\n 本文档基于 ElasticSearch 6.7 版本\n本质与定位Elasticsearch 基于Apache Lucene构建的分布式搜索与分析引擎，采用RESTful API设计\nES可以轻松扩展数以百计的服务器(水平扩展), 用于存储和处理数据. 它可以在很短的时间内存储、搜索和分析海量数据, 通常被作为复杂搜索场景下的核心引擎.\n核心优势ElasticSearch VS RDB\n\n\n\n特性\n传统数据库\nElasticsearch\n\n\n\n检索速度\n秒级\n毫秒级（倒排索引）\n\n\n扩展性\n垂直扩展\n水平扩展（自动分片）\n\n\n数据类型\n结构化为主\n结构化+文本+地理空间\n\n\n容错性\n主从复制\n多副本自动恢复\n\n\n典型使用场景\n实时搜索：电商商品检索、新闻全文搜索\n日志分析：ELK（Elasticsearch+Logstash+Kibana）日志监控\n时序数据：APM（应用性能监控）指标存储\n推荐系统：用户行为数据分析\n\n功能概述\n\n\n功能类别\n关键技术\n实战应用\n\n\n\n数据索引\n_bulk API、近实时(NRT)\n支持每秒万级写入\n\n\n全文检索\n分词器、BM25算法\n精准匹配+模糊搜索\n\n\n聚合分析\nMetric&#x2F;Bucket聚合\n替代部分OLAP场景\n\n\n高可用\n分片副本、集群选举\n99.95% SLA保障\n\n\n数据可视化\nKibana集成\n快速构建监控大屏\n\n\n基本概念术语理解Cluster(集群)\n\n\n\n\n\n\n\n\n集群由一至多个节点组成, 对外提供索引和搜索服务. 一个节点只能加入到一个集群中.\n一个集群是一个或多个节点（服务器）的集合，它们共同持有数据，并提供跨所有节点的联合索引和搜索功能。同一网络中，每个集群要有唯一名称标识，默认情况下该名称为”elasticsearch”。这个名称很重要，水平扩展时, 只需要将新增节点的集群名称设置为要扩容的集群名称, 该节点就会自动加入集群中.\n请确保在不同环境中不要重复使用相同的集群名称，否则可能会导致节点加入错误的集群。例如，你可以使用 logging-dev 、 logging-stage 和 logging-prod 作为开发、暂存和生产集群的名称.\nES做到了去中心化: 访问任一节点等价于访问整个集群.\nNode(节点)\n\n\n\n\n\n\n\n\n节点是集群的一部分，是一个单独的服务器，用于存储数据，并参与集群的索引和搜索功能。就像集群一样，节点通过名称来标识，默认情况下是一个在节点启动时分配的随机通用唯一标识符（UUID）。如果不想使用默认值，可以定义任何想要的节点名称。此名称对于管理非常重要，它帮助你确定网络中的哪些服务器对应于 Elasticsearch 集群中的哪些节点。\n通过配置集群名称，可以将节点设置为加入特定的集群。默认情况下，每个节点都被设置为加入名为 elasticsearch 的集群，这意味着如果你在网络中启动多个节点并且它们能够互相发现，它们将自动形成并加入名为 elasticsearch 的单个集群。\n在一个集群中，你可以拥有任意数量的节点。此外，如果当前网络中没有其他正在运行的 Elasticsearch 节点，启动单个节点将默认形成一个名为 elasticsearch 的新单节点集群。\nShard(分片)\n\n\n\n\n\n\n\n\n单台机器(节点)无法存储大量的索引数据, ES可以把一个完整的索引分成多个分片, 分布到不同的节点上, 从而构成分布式索引.每个分片都是一个Lucene实例, 也就是说每个分片底层都有一个单独的Lucene提供独立的索引和检索服务, 它们可以托管在集群的任一节点上.单个Lucene中可存储文档是有限的,到目前版本，极限是2147483519(=integer.max_value - 128) 个文档. 可使用_cat/shards API 监控分片的大小.\n在创建索引时，可以为每个索引定义分片和副本的数量。在索引创建之后，还可以随时动态更改副本的数量。可以使用 _shrink 和 _split API 更改现有索引的分片数量，但这是一个复杂的任务，预先计划正确的分片数量是最优的方法\n分片的好处:\n\n\n\n\n\n\n\n\n\n允许水平切分&#x2F;扩展集群容量;可在多个分片上进行分布式的、并行的操作, 提高系统的性能和吞吐量.\nReplica(副本)\n\n\n\n\n\n\n\n\nES支持为每个Shard创建多个副本, 相当于索引数据的冗余备份.分片有Primary Shard(主分片)、Replica Shard(副本分片), 建立索引时, 系统会先将索引存储在主分片中, 然后再将主分片中的索引复制到不同的副本中.\n(1) 副本的重要性:\n\n\n\n\n\n\n\n\n\n① 解决单点问题, 提高可用性和容错性: 某个节点失败时服务不受影响, 可以从副本中恢复;② 提高查询效率和查询时的吞吐量: 搜索可以在所有的副本上并行执行, 提高了服务的并发量.\n(2) 使用注意事项:\n\n\n\n\n\n\n\n\n\n主分片在建立索引时设置, 后期不能修改;主分片和副本分片不能存储在同一个节点中 —— 无法保证高可用.默认情况下，Elasticsearch 中的每个索引会被分配 5 个主分片和 1 个副本，这意味着如果你的集群中至少有两个节点，你的索引将有 5 个主分片和另外 5 个副本分片（1 个完整的副本），总共每个索引有 10 个分片\nRiver(数据源)\n\n\n\n\n\n\n\n\n从其他存储方式 (如数据库) 中同步数据到ES的方法, 它是以插件方式存在的一个ES服务, 通过读取river中的数据并把它索引到ES中.官方的river有CouchDB、RabbitMQ、Twitter、Wikipedia等.\nRecovery(数据恢复)\n\n\n\n\n\n\n\n\n又叫数据重新分布: 当有节点加入或退出时, ES会根据机器的负载对索引分片进行重新分配, 挂掉的节点重新启动时也会进行数据恢复.Kibana工具中通过 GET _cat/health?v, 就可以看到集群所处的状态.\nIndex (索引)\n\n\n\n\n\n\n\n\n索引是具有相似结构的文档的集合, 等同于Solr中的集合, 比如可以有一个商品分类索引, 订单索引.每个索引都要有唯一的名称, 名称要小写, 通过索引名称来执行新增、搜索、更新和删除等操作.\nType(类型)\n\n\n\n\n\n\n\n\ntype 是 index 的逻辑分类, 在 6.0.0 版本中弃用在 ES 6.0.0 版本之前, 每个索引中可以定义一个或多个 typetype 和 数据库中的表类似, 比如对博客系统中的数据建立索引, 可以定义用户数据 type , 博客数据 type , 评论数据 type , 每个 document 都必须属于某一个具体的 type , 也就是说每个 document 都有 _type 属性.\nDocument(文档)\n\n\n\n\n\n\n\n\n文档是存储在ES中的一个个JSON格式的字符串, 是ES索引中的最小数据单元, 由 field(字段) 构成.\n文档类似于数据库中表里的数据行Row\nMapping(映射)\n\n\n\n\n\n\n\n\n类似于关系数据库中的Table结构, 每个index都有一个映射: 定义索引中每个字段的类型.所有文档在写进索引之前都会先进行分析, 如何对文本进行分词、哪些词条又会被过滤, 这类行为叫做映射(mapping).映射可以提前定义, 也可以在第一次存储文档时自动识别. 一般由用户自己定义规则.\nField(字段)\n\n\n\n\n\n\n\n\n字段可以是一个简单的值，每个字段都有类型，如：字符串、数字、日期, 也可以是一个数组, 还可以嵌套一个对象或多个对象.字段类似于关系数据库中表数据的列, 每个字段都对应一个类型.可以对 field 指定分词器，指定如何分析字段的值\n\n\n\n术语\n说明\n类比关系型数据库\n\n\n\nIndex\n索引是具有某种相似特征的文档集合\nDatabase 数据库\n\n\nType\n类型用于索引逻辑分区，已在 6.0.0 版本中弃用\nTable 数据表\n\n\nDocument\n文档是可被索引的基本信息单元（JSON）\nRow 数据行\n\n\nField\n文档属性\nColumn 列\n\n\nMapping\n字段类型定义\nSchema 表结构\n\n\nElasticSearch基础模块分层架构\n通过这种模块化设计，Elasticsearch实现了高内聚低耦合的架构，既能处理PB级数据，又能保持毫秒级响应能力。\n核心功能模块\n\n\n模块名称\n核心职责\n关键组件\n\n\n\nCluster Module\n集群管理\n- 节点发现（Zen Discovery）- 主节点选举- 集群状态同步 - 分片分配策略\n\n\nIndex Module\n索引生命周期管理\n- 索引创建&#x2F;删除 - Shard分配控制 - 索引设置（settings） - 索引模板管理\n\n\nSearch Module\n查询处理\n- Query解析（Query DSL）- 分布式搜索执行 - 结果聚合（Aggregations） - 相关性评分（BM25）\n\n\nIngest Module\n数据预处理\n- Pipeline定义 - Processor链（如Grok&#x2F;Date&#x2F;GeoIP） - 数据富化&#x2F;转换\n\n\nMapping Module\n数据类型管理\n- 字段类型定义（text&#x2F;keyword&#x2F;date等） - 动态映射（Dynamic Mapping） - 字段分析器配置\n\n\nPersistence\n数据持久化\n- Translog（事务日志） - Lucene Segment管理 - 数据刷盘（Flush） - 快照&#x2F;恢复\n\n\n一、网络与通信模块Transport Module用于集群内节点之间的内部通信\n实现方式：\n\n基于Netty的TCP通信\n零拷贝压缩传输\n集群拓扑感知路由\n\n关键协议：\njava// 节点间请求类型示例\nenum TransportAction &#123;\n  INDEX_SHARD,       // 分片写入\n  SEARCH_SHARD,      // 分片搜索\n  RECOVERY,          // 数据恢复\n  CLUSTER_STATE_UPDATE // 集群状态更新\n&#125;HTTP ModuleHTTP模块允许通过 JSON over HTTP 的方式访问 ES 的API, HTTP模块本质上是完全异步的，这意味着没有阻塞线程等待响应。使用异步通信进行 HTTP 的好处是解决了 C10k 问题（10k量级的并发连接）\n特性\n\n9200端口提供REST接口\nJSON请求&#x2F;响应处理\n基于Jetty的HTTP服务\n\n使用示例\nbashGET /_cluster/health      # 集群健康检查\nPOST /index/_search       # 执行搜索\nPUT /index/_doc/1         # 文档写入二、数据处理核心模块Lucene ModuleElasticsearch 的存储引擎\n核心功能\n\n倒排索引：Term → Document映射\nDoc Values：列式存储（用于聚合排序）\nSegment合并：后台优化索引结构\n近实时搜索：通过refresh可见新数据\n\nAnalysis Module文本处理流水线：\n\n组件作用\n\nChar Filter：HTML标签去除&#x2F;字符替换\nTokenizer：按规则切分文本（如标准分词器）\nToken Filter：小写转换&#x2F;同义词&#x2F;停用词处理\n\n三、分布式协调模块Discovery Module节点发现机制\n![es_ discovery](https://raw.githubusercontent.com/sunknightzy/picsee/main/Picsee/Downloads/es_ discovery-qIgkjO.png)\n实现方式\n\nZen Discovery（内置）\nEC2（AWS&#x2F;Azure云平台发现插件）\n\nAllocation Module封装了分片分配相关的功能和策略，包括主分片的分配和副分片的分配，本模块由主节点调用\n分片分配策略\n\n平衡策略：基于磁盘使用率&#x2F;分片数量\n感知策略：跨机架&#x2F;可用区部署\n过滤规则：指定节点属性分配\n\n配置示例\nbashPUT _cluster/settings\n&#123;\n  &quot;routing.allocation.awareness.attributes&quot;: &quot;rack_id&quot;,\n  &quot;routing.allocation.exclude._ip&quot;: &quot;192.168.1.*&quot;\n&#125;四、高可用与容错模块Recovery Module故障恢复类型\n\n\n\n场景\n恢复机制\n\n\n\n节点重启\n本地分片恢复\n\n\n主分片故障\n副本提升（Promote Replica）\n\n\n新增副本\n全量复制（Peer Recovery）\n\n\nReplication Module五、资源管理模块Circuit Breaker断路器，防止OOM的保护机制\n\n\n\n断路器类型\n防护目标\n\n\n\nParent Circuit Breaker\n总内存使用\n\n\nField Data\n字段数据缓存\n\n\nRequest\n单个请求内存\n\n\nIn Flight Requests\n传输中请求内存\n\n\nThread Pools线程池分工\n\n\n\n线程池\n用途\n默认队列大小\n\n\n\nsearch\n搜索请求处理\n1000\n\n\nwrite\n索引&#x2F;删除操作\n200\n\n\nrefresh\n索引刷新操作\n无队列\n\n\nsnapshot\n快照操作\n无队列\n\n\n六、扩展性模块Plugin Module扩展机制\n\n分析插件：IK分词器（中文分词）\n发现插件：EC2发现（AWS环境）\n安全插件：OpenID Connect集成\n\n安装示例\nbash./bin/elasticsearch-plugin install analysis-icuScripting Module脚本引擎支持\n\nPainless：默认安全脚本语言\nExpression：高性能数学计算\nMustache：模板渲染\nPython\nJavaScript\n\n使用场景：\nbashGET /_search\n&#123;\n  &quot;script_fields&quot;: &#123;\n    &quot;discounted_price&quot;: &#123;\n      &quot;script&quot;: &#123;\n        &quot;source&quot;: &quot;doc[&#39;price&#39;].value * params.discount&quot;,\n        &quot;params&quot;: &#123;&quot;discount&quot;: 0.8&#125;\n      &#125;\n    &#125;\n  &#125;\n&#125;模块协作示例搜索请求流程\n\n","slug":"ES-01-ElasticSearch基础概念","date":"2018-09-14T03:26:09.000Z","categories_index":"ElasticSearch系列","tags_index":"ElasticSearch","author_index":"GlassCat"},{"id":"b80b88ff323cbedbe9f9c88d7ffb2f8f","title":"Docker搭建Nacos","content":"拉取镜像bashdocker pull nacos/nacos-server:v2.5.1创建数据库和用户在安装 nacos 之前需要安装数据库，并创建 nacos 用户，以此来保存 nacos 的配置信息，这里是安装的 mysql-8.4.5 数据库\nbash# 进入mysql容器\ndocker exec -it mysql /bin/bash\n# 登录mysql\nmysql -uroot -proot\ncreate database nacos;\n# 创建nacos用户，并授权\ncreate user &#39;nacos&#39; identified by &#39;nacos&#39;;\ngrant select , insert, update, delete on nacos.* to &#39;nacos&#39;;\nflush privileges;查看 MySQL 的 Docker 虚拟 IP\nbashdocker inspect --format=&#39;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&#39; mysql\n172.17.0.2启动容器\n\n\n\n\n\n\n\n\nDubbo 用 docker 启动的 nacos 作注册中心, Dubbo 启动失败，连接不上\n问题原因  用 docker 安装的 nacos ,只开启了 8848 端口, 9848 端口没开,导致客户端连接不上,打开就好了。\n原端口: 8848 服务使用 \n偏移量端口1: 9848 客户端gRPC请求服务端端口，用于客户端向服务端发起连接和请求 \n偏移量端口2: 9849 服务端gRPC请求服务端端口，用于服务间同步等\nbashdocker run -d -p 8848:8848 \\\n--name nacos \\\n-e MODE=standalone \\\n-e SPRING_DATASOURCE_PLATFORM=mysql \\\n-e MYSQL_SERVICE_HOST=172.17.0.2 \\\n-e MYSQL_SERVICE_PORT=3306 \\\n-e MYSQL_SERVICE_DB_NAME=nacos \\\n-e MYSQL_SERVICE_USER=nacos \\\n-e MYSQL_SERVICE_PASSWORD=nacos \\\n-e &quot;MYSQL_SERVICE_DB_PARAM=allowPublicKeyRetrieval=true&amp;characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useSSL=false&quot; \\\nnacos/nacos-server:v2.5.1创建 nacos 容器后，复制容器内的配置文件目录到本地文件目录下，并删除容器\nbashdocker cp -a nacos:/home/nacos /Users/oak/docker/\ndocker rm -f nacos重新创建 nacos 容器并挂载本地文件\nshelldocker run -d \\\n-p 8848:8848 \\\n-p 9848:9848 \\\n-p 9849:9849 \\\n--privileged=true \\\n--name nacos \\\n-e JVM_XMS=512m \\\n-e JVM_XMX=512m \\\n-e MODE=standalone \\\n-e SPRING_DATASOURCE_PLATFORM=mysql \\\n-e MYSQL_SERVICE_HOST=172.17.0.2 \\\n-e MYSQL_SERVICE_PORT=3306 \\\n-e MYSQL_SERVICE_DB_NAME=nacos \\\n-e MYSQL_SERVICE_USER=nacos \\\n-e MYSQL_SERVICE_PASSWORD=nacos \\\n-e &quot;MYSQL_SERVICE_DB_PARAM=allowPublicKeyRetrieval=true&amp;characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useSSL=false&quot; \\\n-v /Users/oak/docker/nacos/conf:/home/nacos/conf \\\n-v /Users/oak/docker/nacos/logs:/home/nacos/logs \\\n-v /Users/oak/docker/nacos/data:/home/nacos/data \\\nnacos/nacos-server:v2.5.1查看日志、8848 端口是否正常启动\n打开对应网址 http://host-ip:8848/nacos 【默认账号&#x2F;密码都为 nacos】\n","slug":"Docker搭建Nacos","date":"2018-04-04T06:31:11.000Z","categories_index":"Docker,Nacos","tags_index":"Docker,Nacos","author_index":"GlassCat"},{"id":"61eea4bbe78b91308bbd442a5070c4bd","title":"Docker搭建Kibana+ElasticSearch","content":"创建宿主机目录bashmkdir -p /Users/oak/docker/elasticsearch/config /Users/oak/docker/elasticsearch/data /Users/oak/docker/elasticsearch/log配置修改 elasticsearch.yml 配置文件允许外部访问 (默认配置就行)\nyamlnetwork.host: 0.0.0.0启动ElasticSearchbashdocker run --name elasticsearch \\\n-p 9200:9200 -p 9300:9300 \\\n-e &quot;discovery.type=single-node&quot; \\\n-e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; \\\n-v /Users/oak/docker/elasticsearch/conf/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\\n-v /Users/oak/docker/elasticsearch/data:/usr/share/elasticsearch/data \\\n-d elasticsearch:6.7.1启动Kibanabashdocker run --name kibana \\\n--link elasticsearch \\\n-e ELASTICSEARCH_URL=http://elasticsearch:9200 \\\n-p 5601:5601 -d kibana:6.7.1Elasticsearch 配置文件详解\n展开配置文件 elasticsearch.yaml\nyaml# 所有的配置都可以使用环境变量，如： cluster.name: $&#123;CLUSTER_NAME&#125; 表示环境变量中有一个CLUSTER_NAME变量\n\n# 下面列举一下elasticsearch的可配置项：\n# 1. 集群名称，默认为elasticsearch：\ncluster.name: elasticsearch\n\n# 2. 节点名称，es启动时会自动创建节点名称，但你也可进行配置：\nnode.name: &quot;Franz Kafka&quot;\n\n# 3. 是否作为主节点，每个节点都可以被配置成为主节点，默认值为true：\nnode.master: true\n\n# 4. 是否存储数据，即存储索引片段，默认值为true：\nnode.data: true\n\n# master和data同时配置会产生一些奇异的效果：\n# 1) 当master为false，而data为true时，会对该节点产生严重负荷；\n# 2) 当master为true，而data为false时，该节点作为一个协调者；\n# 3) 当master为false，data也为false时，该节点就变成了一个负载均衡器。\n# 你可以通过连接http://localhost:9200/_cluster/health或者http://localhost:9200/_cluster/nodes，或者使用插件http://github.com/lukas-vlcek/bigdesk或http://mobz.github.com/elasticsearch-head来查看集群状态\n\n# 5. 每个节点都可以定义一些与之关联的通用属性，用于后期集群进行分片时的过滤：\nnode.rack: rack314\n\n# 6. 默认情况下，多个节点可以在同一个安装路径启动，如果你想让你的es只启动一个节点，可以进行如下设置：\nnode.max_local_storage_nodes: 1\n\n# 7. 设置一个索引的分片数量，默认值为5：\nindex.number_of_shards: 5\n# 8. 设置一个索引副本的数量，默认值为1：\nindex.number_of_replicas: 1\n# 当你想要禁用分布式时，你可以进行如下设置：\nindex.number_of_shards: 1\nindex.number_of_replicas: 0\n# 这两个属性的设置直接影响集群中索引和搜索操作的执行。假设你有足够的机器来持有分片和副本，那么可以按如下规则设置这两个值：\n# 1) 拥有更多的分片可以提升索引执行能力，并允许通过机器分发一个大型的索引；\n# 2) 拥有更多的副本能够提升搜索执行能力以及集群能力。\n# 对于一个索引来说，number_of_shards只能设置一次，而number_of_replicas可以使用索引更新设置API在任何时候被增加或者减少。\n# ElasticSearch关注负载均衡、迁移、从节点聚合结果等等。可以尝试多种设计来完成这些功能。\n# 可以连接http://localhost:9200/A/_status来检测索引的状态。\n\n# 9. 配置文件所在的位置，即elasticsearch.yml和logging.yml所在的位置：\npath.conf: /path/to/conf\n\n# 10. 分配给当前节点的索引数据所在的位置：\npath.data: /path/to/data\n# 可以选择的包含一个以上的位置，使得数据在文件级别跨越位置，这样在创建时就有更多的自由路径，如：\n# path.data: /path/to/data1,/path/to/data2\n\n# 11. 临时文件位置：\npath.work: /path/to/work\n\n# 12. 日志文件所在位置：\npath.logs: /path/to/logs\n\n# 13. 插件安装位置：\npath.plugins: /path/to/plugins\n\n# 14. 插件托管位置，若列表中的某一个插件未安装，则节点无法启动：\nplugin.mandatory: mapper-attachments,lang-groovy\n\n# 15. JVM开始交换时，ElasticSearch表现并不好：你需要保障JVM不进行交换，\n#     可以将bootstrap.mlockall设置为true禁止交换：\nbootstrap.mlockall: true\n# 请确保ES_MIN_MEM和ES_MAX_MEM的值是一样的，并且能够为ElasticSearch分配足够的内存，也为系统操作保留足够的内存。\n\n# 16. 默认情况下，ElasticSearch使用0.0.0.0地址，并为http传输开启9200-9300端口，为节点到节点的通信开启9300-9400端口，也可以自行设置IP地址：\nnetwork.bind_host: 192.168.0.1\n\n# 17. publish_host设置其他节点连接此节点的地址，如果不设置的话，则自动获取，publish_host的地址必须为真实地址：\nnetwork.publish_host: 192.168.0.1\n\n# 18. bind_host和publish_host可以一起设置：\nnetwork.host: 192.168.0.1\n\n# 19. 可以定制该节点与其他节点交互的端口：\ntransport.tcp.port: 9300\n\n# 20. 节点间交互时，可以设置是否压缩\ntransport.tcp.compress: true\n\n# 21. 可以为Http传输监听定制端口：\nhttp.port: 9200\n\n# 22. 设置内容的最大长度：\nhttp.max_content_length: 100mb\n\n# 23. 禁止HTTP\nhttp.enabled: false\n\n# 24. 网关允许在所有集群重启后持有集群状态，集群状态的变更都会被保存下来，当第一次启用集群时，可以从网关中读取到状态，默认网关类型（也是推荐的）是local：\ngateway.type: local\n\n# 25. 允许在N个节点启动后恢复过程：\ngateway.recover_after_nodes: 1\n\n# 26. 设置初始化恢复过程的超时时间：\ngateway.recover_after_time: 5m\n\n# 27. 设置该集群中可存在的节点上限：\ngateway.expected_nodes: 2\n\n# 28. 设置一个节点的并发数量，有两种情况，一种是在初始恢复过程中：\ncluster.routing.allocation.node_initial_primaries_recoveries: 4\n# 另一种是在添加、删除节点及调整时：\ncluster.routing.allocation.node_concurrent_recoveries: 2\n\n# 29. 设置恢复时的吞吐量，默认情况下是无限的：\nindices.recovery.max_size_per_sec: 0\n\n# 30. 设置从对等节点恢复片段时打开的流的数量上限：\nindices.recovery.concurrent_streams: 5\n\n# 31. 设置一个集群中主节点的数量，当多于三个节点时，该值可在2-4之间：\ndiscovery.zen.minimum_master_nodes: 1\n\n# 32. 设置ping其他节点时的超时时间，网络比较慢时可将该值设大：\ndiscovery.zen.ping.timeout: 3s\n# http://elasticsearch.org/guide/reference/modules/discovery/zen.html上有更多关于discovery的设置。\n\n# 33. 禁止当前节点发现多个集群节点，默认值为true：\ndiscovery.zen.ping.multicast.enabled: false\n\n# 34. 设置新节点被启动时能够发现的主节点列表（主要用于不同网段机器连接）：\ndiscovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2:port&quot;, &quot;host3[portX-portY]&quot;]\n\n# 35.设置是否可以通过正则或者_all删除或者关闭索引\n# action.destructive_requires_name 默认false 允许 可设置true不允许 \n\n","slug":"Docker搭建Kibana-ElasticSearch","date":"2018-04-04T05:19:11.000Z","categories_index":"Docker,ElasticSearch","tags_index":"Docker,ElasticSearch,Kibana","author_index":"GlassCat"},{"id":"a66406382ee33156e956b3755bbd8fe4","title":"Docker 搭建 zookeeper 单机","content":"创建宿主机目录bashmkdir -p /Users/oak/docker/zookeeper/conf /Users/oak/docker/zookeeper/data /Users/oak/docker/zookeeper/log配置在 conf 目录下新建 zoo.cfg 文件，内容如下\nbashclientPort=2181\ndataDir=/data\ndataLogDir=/datalog\ntickTime=2000\ninitLimit=5\nsyncLimit=2\nmaxClientCnxns=60启动命令创建好宿主机的文件夹和文件，执行下面的命令\nbashdocker run --name zookeeper \\\n--restart=always \\\n-p 2181:2181 -p 2888:2888 -p 3888:3888 \\\n-v /Users/oak/docker/zookeeper/data:/data \\\n-v /Users/oak/docker/zookeeper/log:/datalog \\\n-v /Users/oak/docker/zookeeper/conf/zoo.cfg:/conf/zoo.cfg \\\n--privileged=true \\\n-d zookeeper:3.4.14","slug":"Docker-搭建-zookeeper-单机","date":"2018-04-04T04:39:16.000Z","categories_index":"Docker,Zookeeper","tags_index":"Docker,Zookeeper","author_index":"GlassCat"},{"id":"81e28fe4358465e9bc71f0fe54e70460","title":"Docker 搭建 Redis","content":"创建目录bashmkdir -p /Users/oak/docker/redis/config /Users/oak/docker/redis/data配置文件在 config 目录下新建一份 redis.conf 文件\n一定要注意有一份合格的redis.conf，配置文件中的所有文件夹都要保证在容器中存在，否则启动就报错\n\n点击展开 redis.conf\nbash# Redis configuration file example.\n#\n# Note that in order to read the configuration file, Redis must be\n# started with the file path as first argument:\n#\n# ./redis-server /path/to/redis.conf\n\n# Note on units: when memory size is needed, it is possible to specify\n# it in the usual form of 1k 5GB 4M and so forth:\n#\n# 1k =&gt; 1000 bytes\n# 1kb =&gt; 1024 bytes\n# 1m =&gt; 1000000 bytes\n# 1mb =&gt; 1024*1024 bytes\n# 1g =&gt; 1000000000 bytes\n# 1gb =&gt; 1024*1024*1024 bytes\n#\n# units are case insensitive so 1GB 1Gb 1gB are all the same.\n\n################################## INCLUDES ###################################\n\n# Include one or more other config files here.  This is useful if you\n# have a standard template that goes to all Redis servers but also need\n# to customize a few per-server settings.  Include files can include\n# other files, so use this wisely.\n#\n# Notice option &quot;include&quot; won&#39;t be rewritten by command &quot;CONFIG REWRITE&quot;\n# from admin or Redis Sentinel. Since Redis always uses the last processed\n# line as value of a configuration directive, you&#39;d better put includes\n# at the beginning of this file to avoid overwriting config change at runtime.\n#\n# If instead you are interested in using includes to override configuration\n# options, it is better to use include as the last line.\n#\n# include /path/to/local.conf\n# include /path/to/other.conf\n\n################################## MODULES #####################################\n\n# Load modules at startup. If the server is not able to load modules\n# it will abort. It is possible to use multiple loadmodule directives.\n#\n# loadmodule /path/to/my_module.so\n# loadmodule /path/to/other_module.so\n\n################################## NETWORK #####################################\n\n# By default, if no &quot;bind&quot; configuration directive is specified, Redis listens\n# for connections from all the network interfaces available on the server.\n# It is possible to listen to just one or multiple selected interfaces using\n# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.\n#\n# Examples:\n#\n# bind 192.168.1.100 10.0.0.1\n# bind 127.0.0.1 ::1\n#\n# ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the\n# internet, binding to all the interfaces is dangerous and will expose the\n# instance to everybody on the internet. So by default we uncomment the\n# following bind directive, that will force Redis to listen only into\n# the IPv4 loopback interface address (this means Redis will be able to\n# accept connections only from clients running into the same computer it\n# is running).\n#\n# IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES\n# JUST COMMENT THE FOLLOWING LINE.\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nbind 0.0.0.0\n\n# Protected mode is a layer of security protection, in order to avoid that\n# Redis instances left open on the internet are accessed and exploited.\n#\n# When protected mode is on and if:\n#\n# 1) The server is not binding explicitly to a set of addresses using the\n#    &quot;bind&quot; directive.\n# 2) No password is configured.\n#\n# The server only accepts connections from clients connecting from the\n# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain\n# sockets.\n#\n# By default protected mode is enabled. You should disable it only if\n# you are sure you want clients from other hosts to connect to Redis\n# even if no authentication is configured, nor a specific set of interfaces\n# are explicitly listed using the &quot;bind&quot; directive.\nprotected-mode no\n\n# Accept connections on the specified port, default is 6379 (IANA #815344).\n# If port 0 is specified Redis will not listen on a TCP socket.\nport 6379\n\n# TCP listen() backlog.\n#\n# In high requests-per-second environments you need an high backlog in order\n# to avoid slow clients connections issues. Note that the Linux kernel\n# will silently truncate it to the value of /proc/sys/net/core/somaxconn so\n# make sure to raise both the value of somaxconn and tcp_max_syn_backlog\n# in order to get the desired effect.\ntcp-backlog 511\n\n# Unix socket.\n#\n# Specify the path for the Unix socket that will be used to listen for\n# incoming connections. There is no default, so Redis will not listen\n# on a unix socket when not specified.\n#\n# unixsocket /tmp/redis.sock\n# unixsocketperm 700\n\n# Close the connection after a client is idle for N seconds (0 to disable)\ntimeout 0\n\n# TCP keepalive.\n#\n# If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence\n# of communication. This is useful for two reasons:\n#\n# 1) Detect dead peers.\n# 2) Take the connection alive from the point of view of network\n#    equipment in the middle.\n#\n# On Linux, the specified value (in seconds) is the period used to send ACKs.\n# Note that to close the connection the double of the time is needed.\n# On other kernels the period depends on the kernel configuration.\n#\n# A reasonable value for this option is 300 seconds, which is the new\n# Redis default starting with Redis 3.2.1.\ntcp-keepalive 300\n\n################################# GENERAL #####################################\n\n# By default Redis does not run as a daemon. Use &#39;yes&#39; if you need it.\n# Note that Redis will write a pid file in /usr/local/var/run/redis.pid when daemonized.\ndaemonize no\n\n# If you run Redis from upstart or systemd, Redis can interact with your\n# supervision tree. Options:\n#   supervised no      - no supervision interaction\n#   supervised upstart - signal upstart by putting Redis into SIGSTOP mode\n#   supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET\n#   supervised auto    - detect upstart or systemd method based on\n#                        UPSTART_JOB or NOTIFY_SOCKET environment variables\n# Note: these supervision methods only signal &quot;process is ready.&quot;\n#       They do not enable continuous liveness pings back to your supervisor.\nsupervised no\n\n# If a pid file is specified, Redis writes it where specified at startup\n# and removes it at exit.\n#\n# When the server runs non daemonized, no pid file is created if none is\n# specified in the configuration. When the server is daemonized, the pid file\n# is used even if not specified, defaulting to &quot;/usr/local/var/run/redis.pid&quot;.\n#\n# Creating a pid file is best effort: if Redis is not able to create it\n# nothing bad happens, the server will start and run normally.\npidfile /var/run/redis_6379.pid\n\n# Specify the server verbosity level.\n# This can be one of:\n# debug (a lot of information, useful for development/testing)\n# verbose (many rarely useful info, but not a mess like the debug level)\n# notice (moderately verbose, what you want in production probably)\n# warning (only very important / critical messages are logged)\nloglevel notice\n\n# Specify the log file name. Also the empty string can be used to force\n# Redis to log on the standard output. Note that if you use standard\n# output for logging but daemonize, logs will be sent to /dev/null\nlogfile &quot;&quot;\n\n# To enable logging to the system logger, just set &#39;syslog-enabled&#39; to yes,\n# and optionally update the other syslog parameters to suit your needs.\n# syslog-enabled no\n\n# Specify the syslog identity.\n# syslog-ident redis\n\n# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.\n# syslog-facility local0\n\n# Set the number of databases. The default database is DB 0, you can select\n# a different one on a per-connection basis using SELECT &lt;dbid&gt; where\n# dbid is a number between 0 and &#39;databases&#39;-1\ndatabases 16\n\n# By default Redis shows an ASCII art logo only when started to log to the\n# standard output and if the standard output is a TTY. Basically this means\n# that normally a logo is displayed only in interactive sessions.\n#\n# However it is possible to force the pre-4.0 behavior and always show a\n# ASCII art logo in startup logs by setting the following option to yes.\nalways-show-logo yes\n\n################################ SNAPSHOTTING  ################################\n#\n# Save the DB on disk:\n#\n#   save &lt;seconds&gt; &lt;changes&gt;\n#\n#   Will save the DB if both the given number of seconds and the given\n#   number of write operations against the DB occurred.\n#\n#   In the example below the behaviour will be to save:\n#   after 900 sec (15 min) if at least 1 key changed\n#   after 300 sec (5 min) if at least 10 keys changed\n#   after 60 sec if at least 10000 keys changed\n#\n#   Note: you can disable saving completely by commenting out all &quot;save&quot; lines.\n#\n#   It is also possible to remove all the previously configured save\n#   points by adding a save directive with a single empty string argument\n#   like in the following example:\n#\n#   save &quot;&quot;\n\nsave 900 1\nsave 300 10\nsave 60 10000\n\n# By default Redis will stop accepting writes if RDB snapshots are enabled\n# (at least one save point) and the latest background save failed.\n# This will make the user aware (in a hard way) that data is not persisting\n# on disk properly, otherwise chances are that no one will notice and some\n# disaster will happen.\n#\n# If the background saving process will start working again Redis will\n# automatically allow writes again.\n#\n# However if you have setup your proper monitoring of the Redis server\n# and persistence, you may want to disable this feature so that Redis will\n# continue to work as usual even if there are problems with disk,\n# permissions, and so forth.\nstop-writes-on-bgsave-error yes\n\n# Compress string objects using LZF when dump .rdb databases?\n# For default that&#39;s set to &#39;yes&#39; as it&#39;s almost always a win.\n# If you want to save some CPU in the saving child set it to &#39;no&#39; but\n# the dataset will likely be bigger if you have compressible values or keys.\nrdbcompression yes\n\n# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.\n# This makes the format more resistant to corruption but there is a performance\n# hit to pay (around 10%) when saving and loading RDB files, so you can disable it\n# for maximum performances.\n#\n# RDB files created with checksum disabled have a checksum of zero that will\n# tell the loading code to skip the check.\nrdbchecksum yes\n\n# The filename where to dump the DB\ndbfilename dump.rdb\n\n# The working directory.\n#\n# The DB will be written inside this directory, with the filename specified\n# above using the &#39;dbfilename&#39; configuration directive.\n#\n# The Append Only File will also be created inside this directory.\n#\n# Note that you must specify a directory here, not a file name.\ndir ./\n\n################################# REPLICATION #################################\n\n# Master-Replica replication. Use replicaof to make a Redis instance a copy of\n# another Redis server. A few things to understand ASAP about Redis replication.\n#\n#   +------------------+      +---------------+\n#   |      Master      | ---&gt; |    Replica    |\n#   | (receive writes) |      |  (exact copy) |\n#   +------------------+      +---------------+\n#\n# 1) Redis replication is asynchronous, but you can configure a master to\n#    stop accepting writes if it appears to be not connected with at least\n#    a given number of replicas.\n# 2) Redis replicas are able to perform a partial resynchronization with the\n#    master if the replication link is lost for a relatively small amount of\n#    time. You may want to configure the replication backlog size (see the next\n#    sections of this file) with a sensible value depending on your needs.\n# 3) Replication is automatic and does not need user intervention. After a\n#    network partition replicas automatically try to reconnect to masters\n#    and resynchronize with them.\n#\n# replicaof &lt;masterip&gt; &lt;masterport&gt;\n\n# If the master is password protected (using the &quot;requirepass&quot; configuration\n# directive below) it is possible to tell the replica to authenticate before\n# starting the replication synchronization process, otherwise the master will\n# refuse the replica request.\n#\n# masterauth &lt;master-password&gt;\n\n# When a replica loses its connection with the master, or when the replication\n# is still in progress, the replica can act in two different ways:\n#\n# 1) if replica-serve-stale-data is set to &#39;yes&#39; (the default) the replica will\n#    still reply to client requests, possibly with out of date data, or the\n#    data set may just be empty if this is the first synchronization.\n#\n# 2) if replica-serve-stale-data is set to &#39;no&#39; the replica will reply with\n#    an error &quot;SYNC with master in progress&quot; to all the kind of commands\n#    but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,\n#    SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,\n#    COMMAND, POST, HOST: and LATENCY.\n#\nreplica-serve-stale-data yes\n\n# You can configure a replica instance to accept writes or not. Writing against\n# a replica instance may be useful to store some ephemeral data (because data\n# written on a replica will be easily deleted after resync with the master) but\n# may also cause problems if clients are writing to it because of a\n# misconfiguration.\n#\n# Since Redis 2.6 by default replicas are read-only.\n#\n# Note: read only replicas are not designed to be exposed to untrusted clients\n# on the internet. It&#39;s just a protection layer against misuse of the instance.\n# Still a read only replica exports by default all the administrative commands\n# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve\n# security of read only replicas using &#39;rename-command&#39; to shadow all the\n# administrative / dangerous commands.\nreplica-read-only yes\n\n# Replication SYNC strategy: disk or socket.\n#\n# -------------------------------------------------------\n# WARNING: DISKLESS REPLICATION IS EXPERIMENTAL CURRENTLY\n# -------------------------------------------------------\n#\n# New replicas and reconnecting replicas that are not able to continue the replication\n# process just receiving differences, need to do what is called a &quot;full\n# synchronization&quot;. An RDB file is transmitted from the master to the replicas.\n# The transmission can happen in two different ways:\n#\n# 1) Disk-backed: The Redis master creates a new process that writes the RDB\n#                 file on disk. Later the file is transferred by the parent\n#                 process to the replicas incrementally.\n# 2) Diskless: The Redis master creates a new process that directly writes the\n#              RDB file to replica sockets, without touching the disk at all.\n#\n# With disk-backed replication, while the RDB file is generated, more replicas\n# can be queued and served with the RDB file as soon as the current child producing\n# the RDB file finishes its work. With diskless replication instead once\n# the transfer starts, new replicas arriving will be queued and a new transfer\n# will start when the current one terminates.\n#\n# When diskless replication is used, the master waits a configurable amount of\n# time (in seconds) before starting the transfer in the hope that multiple replicas\n# will arrive and the transfer can be parallelized.\n#\n# With slow disks and fast (large bandwidth) networks, diskless replication\n# works better.\nrepl-diskless-sync no\n\n# When diskless replication is enabled, it is possible to configure the delay\n# the server waits in order to spawn the child that transfers the RDB via socket\n# to the replicas.\n#\n# This is important since once the transfer starts, it is not possible to serve\n# new replicas arriving, that will be queued for the next RDB transfer, so the server\n# waits a delay in order to let more replicas arrive.\n#\n# The delay is specified in seconds, and by default is 5 seconds. To disable\n# it entirely just set it to 0 seconds and the transfer will start ASAP.\nrepl-diskless-sync-delay 5\n\n# Replicas send PINGs to server in a predefined interval. It&#39;s possible to change\n# this interval with the repl_ping_replica_period option. The default value is 10\n# seconds.\n#\n# repl-ping-replica-period 10\n\n# The following option sets the replication timeout for:\n#\n# 1) Bulk transfer I/O during SYNC, from the point of view of replica.\n# 2) Master timeout from the point of view of replicas (data, pings).\n# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).\n#\n# It is important to make sure that this value is greater than the value\n# specified for repl-ping-replica-period otherwise a timeout will be detected\n# every time there is low traffic between the master and the replica.\n#\n# repl-timeout 60\n\n# Disable TCP_NODELAY on the replica socket after SYNC?\n#\n# If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and\n# less bandwidth to send data to replicas. But this can add a delay for\n# the data to appear on the replica side, up to 40 milliseconds with\n# Linux kernels using a default configuration.\n#\n# If you select &quot;no&quot; the delay for data to appear on the replica side will\n# be reduced but more bandwidth will be used for replication.\n#\n# By default we optimize for low latency, but in very high traffic conditions\n# or when the master and replicas are many hops away, turning this to &quot;yes&quot; may\n# be a good idea.\nrepl-disable-tcp-nodelay no\n\n# Set the replication backlog size. The backlog is a buffer that accumulates\n# replica data when replicas are disconnected for some time, so that when a replica\n# wants to reconnect again, often a full resync is not needed, but a partial\n# resync is enough, just passing the portion of data the replica missed while\n# disconnected.\n#\n# The bigger the replication backlog, the longer the time the replica can be\n# disconnected and later be able to perform a partial resynchronization.\n#\n# The backlog is only allocated once there is at least a replica connected.\n#\n# repl-backlog-size 1mb\n\n# After a master has no longer connected replicas for some time, the backlog\n# will be freed. The following option configures the amount of seconds that\n# need to elapse, starting from the time the last replica disconnected, for\n# the backlog buffer to be freed.\n#\n# Note that replicas never free the backlog for timeout, since they may be\n# promoted to masters later, and should be able to correctly &quot;partially\n# resynchronize&quot; with the replicas: hence they should always accumulate backlog.\n#\n# A value of 0 means to never release the backlog.\n#\n# repl-backlog-ttl 3600\n\n# The replica priority is an integer number published by Redis in the INFO output.\n# It is used by Redis Sentinel in order to select a replica to promote into a\n# master if the master is no longer working correctly.\n#\n# A replica with a low priority number is considered better for promotion, so\n# for instance if there are three replicas with priority 10, 100, 25 Sentinel will\n# pick the one with priority 10, that is the lowest.\n#\n# However a special priority of 0 marks the replica as not able to perform the\n# role of master, so a replica with priority of 0 will never be selected by\n# Redis Sentinel for promotion.\n#\n# By default the priority is 100.\nreplica-priority 100\n\n# It is possible for a master to stop accepting writes if there are less than\n# N replicas connected, having a lag less or equal than M seconds.\n#\n# The N replicas need to be in &quot;online&quot; state.\n#\n# The lag in seconds, that must be &lt;= the specified value, is calculated from\n# the last ping received from the replica, that is usually sent every second.\n#\n# This option does not GUARANTEE that N replicas will accept the write, but\n# will limit the window of exposure for lost writes in case not enough replicas\n# are available, to the specified number of seconds.\n#\n# For example to require at least 3 replicas with a lag &lt;= 10 seconds use:\n#\n# min-replicas-to-write 3\n# min-replicas-max-lag 10\n#\n# Setting one or the other to 0 disables the feature.\n#\n# By default min-replicas-to-write is set to 0 (feature disabled) and\n# min-replicas-max-lag is set to 10.\n\n# A Redis master is able to list the address and port of the attached\n# replicas in different ways. For example the &quot;INFO replication&quot; section\n# offers this information, which is used, among other tools, by\n# Redis Sentinel in order to discover replica instances.\n# Another place where this info is available is in the output of the\n# &quot;ROLE&quot; command of a master.\n#\n# The listed IP and address normally reported by a replica is obtained\n# in the following way:\n#\n#   IP: The address is auto detected by checking the peer address\n#   of the socket used by the replica to connect with the master.\n#\n#   Port: The port is communicated by the replica during the replication\n#   handshake, and is normally the port that the replica is using to\n#   listen for connections.\n#\n# However when port forwarding or Network Address Translation (NAT) is\n# used, the replica may be actually reachable via different IP and port\n# pairs. The following two options can be used by a replica in order to\n# report to its master a specific set of IP and port, so that both INFO\n# and ROLE will report those values.\n#\n# There is no need to use both the options if you need to override just\n# the port or the IP address.\n#\n# replica-announce-ip 5.5.5.5\n# replica-announce-port 1234\n\n################################## SECURITY ###################################\n\n# Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other\n# commands.  This might be useful in environments in which you do not trust\n# others with access to the host running redis-server.\n#\n# This should stay commented out for backward compatibility and because most\n# people do not need auth (e.g. they run their own servers).\n#\n# Warning: since Redis is pretty fast an outside user can try up to\n# 150k passwords per second against a good box. This means that you should\n# use a very strong password otherwise it will be very easy to break.\n#\n# requirepass foobared\n\n# Command renaming.\n#\n# It is possible to change the name of dangerous commands in a shared\n# environment. For instance the CONFIG command may be renamed into something\n# hard to guess so that it will still be available for internal-use tools\n# but not available for general clients.\n#\n# Example:\n#\n# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52\n#\n# It is also possible to completely kill a command by renaming it into\n# an empty string:\n#\n# rename-command CONFIG &quot;&quot;\n#\n# Please note that changing the name of commands that are logged into the\n# AOF file or transmitted to replicas may cause problems.\n\n################################### CLIENTS ####################################\n\n# Set the max number of connected clients at the same time. By default\n# this limit is set to 10000 clients, however if the Redis server is not\n# able to configure the process file limit to allow for the specified limit\n# the max number of allowed clients is set to the current file limit\n# minus 32 (as Redis reserves a few file descriptors for internal uses).\n#\n# Once the limit is reached Redis will close all the new connections sending\n# an error &#39;max number of clients reached&#39;.\n#\n# maxclients 10000\n\n############################## MEMORY MANAGEMENT ################################\n\n# Set a memory usage limit to the specified amount of bytes.\n# When the memory limit is reached Redis will try to remove keys\n# according to the eviction policy selected (see maxmemory-policy).\n#\n# If Redis can&#39;t remove keys according to the policy, or if the policy is\n# set to &#39;noeviction&#39;, Redis will start to reply with errors to commands\n# that would use more memory, like SET, LPUSH, and so on, and will continue\n# to reply to read-only commands like GET.\n#\n# This option is usually useful when using Redis as an LRU or LFU cache, or to\n# set a hard memory limit for an instance (using the &#39;noeviction&#39; policy).\n#\n# WARNING: If you have replicas attached to an instance with maxmemory on,\n# the size of the output buffers needed to feed the replicas are subtracted\n# from the used memory count, so that network problems / resyncs will\n# not trigger a loop where keys are evicted, and in turn the output\n# buffer of replicas is full with DELs of keys evicted triggering the deletion\n# of more keys, and so forth until the database is completely emptied.\n#\n# In short... if you have replicas attached it is suggested that you set a lower\n# limit for maxmemory so that there is some free RAM on the system for replica\n# output buffers (but this is not needed if the policy is &#39;noeviction&#39;).\n#\n# maxmemory &lt;bytes&gt;\n\n# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory\n# is reached. You can select among five behaviors:\n#\n# volatile-lru -&gt; Evict using approximated LRU among the keys with an expire set.\n# allkeys-lru -&gt; Evict any key using approximated LRU.\n# volatile-lfu -&gt; Evict using approximated LFU among the keys with an expire set.\n# allkeys-lfu -&gt; Evict any key using approximated LFU.\n# volatile-random -&gt; Remove a random key among the ones with an expire set.\n# allkeys-random -&gt; Remove a random key, any key.\n# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)\n# noeviction -&gt; Don&#39;t evict anything, just return an error on write operations.\n#\n# LRU means Least Recently Used\n# LFU means Least Frequently Used\n#\n# Both LRU, LFU and volatile-ttl are implemented using approximated\n# randomized algorithms.\n#\n# Note: with any of the above policies, Redis will return an error on write\n#       operations, when there are no suitable keys for eviction.\n#\n#       At the date of writing these commands are: set setnx setex append\n#       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd\n#       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby\n#       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby\n#       getset mset msetnx exec sort\n#\n# The default is:\n#\n# maxmemory-policy noeviction\n\n# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated\n# algorithms (in order to save memory), so you can tune it for speed or\n# accuracy. For default Redis will check five keys and pick the one that was\n# used less recently, you can change the sample size using the following\n# configuration directive.\n#\n# The default of 5 produces good enough results. 10 Approximates very closely\n# true LRU but costs more CPU. 3 is faster but not very accurate.\n#\n# maxmemory-samples 5\n\n# Starting from Redis 5, by default a replica will ignore its maxmemory setting\n# (unless it is promoted to master after a failover or manually). It means\n# that the eviction of keys will be just handled by the master, sending the\n# DEL commands to the replica as keys evict in the master side.\n#\n# This behavior ensures that masters and replicas stay consistent, and is usually\n# what you want, however if your replica is writable, or you want the replica to have\n# a different memory setting, and you are sure all the writes performed to the\n# replica are idempotent, then you may change this default (but be sure to understand\n# what you are doing).\n#\n# Note that since the replica by default does not evict, it may end using more\n# memory than the one set via maxmemory (there are certain buffers that may\n# be larger on the replica, or data structures may sometimes take more memory and so\n# forth). So make sure you monitor your replicas and make sure they have enough\n# memory to never hit a real out-of-memory condition before the master hits\n# the configured maxmemory setting.\n#\n# replica-ignore-maxmemory yes\n\n############################# LAZY FREEING ####################################\n\n# Redis has two primitives to delete keys. One is called DEL and is a blocking\n# deletion of the object. It means that the server stops processing new commands\n# in order to reclaim all the memory associated with an object in a synchronous\n# way. If the key deleted is associated with a small object, the time needed\n# in order to execute the DEL command is very small and comparable to most other\n# O(1) or O(log_N) commands in Redis. However if the key is associated with an\n# aggregated value containing millions of elements, the server can block for\n# a long time (even seconds) in order to complete the operation.\n#\n# For the above reasons Redis also offers non blocking deletion primitives\n# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and\n# FLUSHDB commands, in order to reclaim memory in background. Those commands\n# are executed in constant time. Another thread will incrementally free the\n# object in the background as fast as possible.\n#\n# DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.\n# It&#39;s up to the design of the application to understand when it is a good\n# idea to use one or the other. However the Redis server sometimes has to\n# delete keys or flush the whole database as a side effect of other operations.\n# Specifically Redis deletes objects independently of a user call in the\n# following scenarios:\n#\n# 1) On eviction, because of the maxmemory and maxmemory policy configurations,\n#    in order to make room for new data, without going over the specified\n#    memory limit.\n# 2) Because of expire: when a key with an associated time to live (see the\n#    EXPIRE command) must be deleted from memory.\n# 3) Because of a side effect of a command that stores data on a key that may\n#    already exist. For example the RENAME command may delete the old key\n#    content when it is replaced with another one. Similarly SUNIONSTORE\n#    or SORT with STORE option may delete existing keys. The SET command\n#    itself removes any old content of the specified key in order to replace\n#    it with the specified string.\n# 4) During replication, when a replica performs a full resynchronization with\n#    its master, the content of the whole database is removed in order to\n#    load the RDB file just transferred.\n#\n# In all the above cases the default is to delete objects in a blocking way,\n# like if DEL was called. However you can configure each case specifically\n# in order to instead release memory in a non-blocking way like if UNLINK\n# was called, using the following configuration directives:\n\nlazyfree-lazy-eviction no\nlazyfree-lazy-expire no\nlazyfree-lazy-server-del no\nreplica-lazy-flush no\n\n############################## APPEND ONLY MODE ###############################\n\n# By default Redis asynchronously dumps the dataset on disk. This mode is\n# good enough in many applications, but an issue with the Redis process or\n# a power outage may result into a few minutes of writes lost (depending on\n# the configured save points).\n#\n# The Append Only File is an alternative persistence mode that provides\n# much better durability. For instance using the default data fsync policy\n# (see later in the config file) Redis can lose just one second of writes in a\n# dramatic event like a server power outage, or a single write if something\n# wrong with the Redis process itself happens, but the operating system is\n# still running correctly.\n#\n# AOF and RDB persistence can be enabled at the same time without problems.\n# If the AOF is enabled on startup Redis will load the AOF, that is the file\n# with the better durability guarantees.\n#\n# Please check http://redis.io/topics/persistence for more information.\n\nappendonly yes\n\n# The name of the append only file (default: &quot;appendonly.aof&quot;)\n\nappendfilename &quot;appendonly.aof&quot;\n\n# The fsync() call tells the Operating System to actually write data on disk\n# instead of waiting for more data in the output buffer. Some OS will really flush\n# data on disk, some other OS will just try to do it ASAP.\n#\n# Redis supports three different modes:\n#\n# no: don&#39;t fsync, just let the OS flush the data when it wants. Faster.\n# always: fsync after every write to the append only log. Slow, Safest.\n# everysec: fsync only one time every second. Compromise.\n#\n# The default is &quot;everysec&quot;, as that&#39;s usually the right compromise between\n# speed and data safety. It&#39;s up to you to understand if you can relax this to\n# &quot;no&quot; that will let the operating system flush the output buffer when\n# it wants, for better performances (but if you can live with the idea of\n# some data loss consider the default persistence mode that&#39;s snapshotting),\n# or on the contrary, use &quot;always&quot; that&#39;s very slow but a bit safer than\n# everysec.\n#\n# More details please check the following article:\n# http://antirez.com/post/redis-persistence-demystified.html\n#\n# If unsure, use &quot;everysec&quot;.\n\n# appendfsync always\nappendfsync everysec\n# appendfsync no\n\n# When the AOF fsync policy is set to always or everysec, and a background\n# saving process (a background save or AOF log background rewriting) is\n# performing a lot of I/O against the disk, in some Linux configurations\n# Redis may block too long on the fsync() call. Note that there is no fix for\n# this currently, as even performing fsync in a different thread will block\n# our synchronous write(2) call.\n#\n# In order to mitigate this problem it&#39;s possible to use the following option\n# that will prevent fsync() from being called in the main process while a\n# BGSAVE or BGREWRITEAOF is in progress.\n#\n# This means that while another child is saving, the durability of Redis is\n# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is\n# possible to lose up to 30 seconds of log in the worst scenario (with the\n# default Linux settings).\n#\n# If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as\n# &quot;no&quot; that is the safest pick from the point of view of durability.\n\nno-appendfsync-on-rewrite no\n\n# Automatic rewrite of the append only file.\n# Redis is able to automatically rewrite the log file implicitly calling\n# BGREWRITEAOF when the AOF log size grows by the specified percentage.\n#\n# This is how it works: Redis remembers the size of the AOF file after the\n# latest rewrite (if no rewrite has happened since the restart, the size of\n# the AOF at startup is used).\n#\n# This base size is compared to the current size. If the current size is\n# bigger than the specified percentage, the rewrite is triggered. Also\n# you need to specify a minimal size for the AOF file to be rewritten, this\n# is useful to avoid rewriting the AOF file even if the percentage increase\n# is reached but it is still pretty small.\n#\n# Specify a percentage of zero in order to disable the automatic AOF\n# rewrite feature.\n\nauto-aof-rewrite-percentage 100\nauto-aof-rewrite-min-size 64mb\n\n# An AOF file may be found to be truncated at the end during the Redis\n# startup process, when the AOF data gets loaded back into memory.\n# This may happen when the system where Redis is running\n# crashes, especially when an ext4 filesystem is mounted without the\n# data=ordered option (however this can&#39;t happen when Redis itself\n# crashes or aborts but the operating system still works correctly).\n#\n# Redis can either exit with an error when this happens, or load as much\n# data as possible (the default now) and start if the AOF file is found\n# to be truncated at the end. The following option controls this behavior.\n#\n# If aof-load-truncated is set to yes, a truncated AOF file is loaded and\n# the Redis server starts emitting a log to inform the user of the event.\n# Otherwise if the option is set to no, the server aborts with an error\n# and refuses to start. When the option is set to no, the user requires\n# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart\n# the server.\n#\n# Note that if the AOF file will be found to be corrupted in the middle\n# the server will still exit with an error. This option only applies when\n# Redis will try to read more data from the AOF file but not enough bytes\n# will be found.\naof-load-truncated yes\n\n# When rewriting the AOF file, Redis is able to use an RDB preamble in the\n# AOF file for faster rewrites and recoveries. When this option is turned\n# on the rewritten AOF file is composed of two different stanzas:\n#\n#   [RDB file][AOF tail]\n#\n# When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;\n# string and loads the prefixed RDB file, and continues loading the AOF\n# tail.\naof-use-rdb-preamble yes\n\n################################ LUA SCRIPTING  ###############################\n\n# Max execution time of a Lua script in milliseconds.\n#\n# If the maximum execution time is reached Redis will log that a script is\n# still in execution after the maximum allowed time and will start to\n# reply to queries with an error.\n#\n# When a long running script exceeds the maximum execution time only the\n# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be\n# used to stop a script that did not yet called write commands. The second\n# is the only way to shut down the server in the case a write command was\n# already issued by the script but the user doesn&#39;t want to wait for the natural\n# termination of the script.\n#\n# Set it to 0 or a negative value for unlimited execution without warnings.\nlua-time-limit 5000\n\n################################ REDIS CLUSTER  ###############################\n#\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n# WARNING EXPERIMENTAL: Redis Cluster is considered to be stable code, however\n# in order to mark it as &quot;mature&quot; we need to wait for a non trivial percentage\n# of users to deploy it in production.\n# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n#\n# Normal Redis instances can&#39;t be part of a Redis Cluster; only nodes that are\n# started as cluster nodes can. In order to start a Redis instance as a\n# cluster node enable the cluster support uncommenting the following:\n#\n# cluster-enabled yes\n\n# Every cluster node has a cluster configuration file. This file is not\n# intended to be edited by hand. It is created and updated by Redis nodes.\n# Every Redis Cluster node requires a different cluster configuration file.\n# Make sure that instances running in the same system do not have\n# overlapping cluster configuration file names.\n#\n# cluster-config-file nodes-6379.conf\n\n# Cluster node timeout is the amount of milliseconds a node must be unreachable\n# for it to be considered in failure state.\n# Most other internal time limits are multiple of the node timeout.\n#\n# cluster-node-timeout 15000\n\n# A replica of a failing master will avoid to start a failover if its data\n# looks too old.\n#\n# There is no simple way for a replica to actually have an exact measure of\n# its &quot;data age&quot;, so the following two checks are performed:\n#\n# 1) If there are multiple replicas able to failover, they exchange messages\n#    in order to try to give an advantage to the replica with the best\n#    replication offset (more data from the master processed).\n#    Replicas will try to get their rank by offset, and apply to the start\n#    of the failover a delay proportional to their rank.\n#\n# 2) Every single replica computes the time of the last interaction with\n#    its master. This can be the last ping or command received (if the master\n#    is still in the &quot;connected&quot; state), or the time that elapsed since the\n#    disconnection with the master (if the replication link is currently down).\n#    If the last interaction is too old, the replica will not try to failover\n#    at all.\n#\n# The point &quot;2&quot; can be tuned by user. Specifically a replica will not perform\n# the failover if, since the last interaction with the master, the time\n# elapsed is greater than:\n#\n#   (node-timeout * replica-validity-factor) + repl-ping-replica-period\n#\n# So for example if node-timeout is 30 seconds, and the replica-validity-factor\n# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the\n# replica will not try to failover if it was not able to talk with the master\n# for longer than 310 seconds.\n#\n# A large replica-validity-factor may allow replicas with too old data to failover\n# a master, while a too small value may prevent the cluster from being able to\n# elect a replica at all.\n#\n# For maximum availability, it is possible to set the replica-validity-factor\n# to a value of 0, which means, that replicas will always try to failover the\n# master regardless of the last time they interacted with the master.\n# (However they&#39;ll always try to apply a delay proportional to their\n# offset rank).\n#\n# Zero is the only value able to guarantee that when all the partitions heal\n# the cluster will always be able to continue.\n#\n# cluster-replica-validity-factor 10\n\n# Cluster replicas are able to migrate to orphaned masters, that are masters\n# that are left without working replicas. This improves the cluster ability\n# to resist to failures as otherwise an orphaned master can&#39;t be failed over\n# in case of failure if it has no working replicas.\n#\n# Replicas migrate to orphaned masters only if there are still at least a\n# given number of other working replicas for their old master. This number\n# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a replica\n# will migrate only if there is at least 1 other working replica for its master\n# and so forth. It usually reflects the number of replicas you want for every\n# master in your cluster.\n#\n# Default is 1 (replicas migrate only if their masters remain with at least\n# one replica). To disable migration just set it to a very large value.\n# A value of 0 can be set but is useful only for debugging and dangerous\n# in production.\n#\n# cluster-migration-barrier 1\n\n# By default Redis Cluster nodes stop accepting queries if they detect there\n# is at least an hash slot uncovered (no available node is serving it).\n# This way if the cluster is partially down (for example a range of hash slots\n# are no longer covered) all the cluster becomes, eventually, unavailable.\n# It automatically returns available as soon as all the slots are covered again.\n#\n# However sometimes you want the subset of the cluster which is working,\n# to continue to accept queries for the part of the key space that is still\n# covered. In order to do so, just set the cluster-require-full-coverage\n# option to no.\n#\n# cluster-require-full-coverage yes\n\n# This option, when set to yes, prevents replicas from trying to failover its\n# master during master failures. However the master can still perform a\n# manual failover, if forced to do so.\n#\n# This is useful in different scenarios, especially in the case of multiple\n# data center operations, where we want one side to never be promoted if not\n# in the case of a total DC failure.\n#\n# cluster-replica-no-failover no\n\n# In order to setup your cluster make sure to read the documentation\n# available at http://redis.io web site.\n\n########################## CLUSTER DOCKER/NAT support  ########################\n\n# In certain deployments, Redis Cluster nodes address discovery fails, because\n# addresses are NAT-ted or because ports are forwarded (the typical case is\n# Docker and other containers).\n#\n# In order to make Redis Cluster working in such environments, a static\n# configuration where each node knows its public address is needed. The\n# following two options are used for this scope, and are:\n#\n# * cluster-announce-ip\n# * cluster-announce-port\n# * cluster-announce-bus-port\n#\n# Each instruct the node about its address, client port, and cluster message\n# bus port. The information is then published in the header of the bus packets\n# so that other nodes will be able to correctly map the address of the node\n# publishing the information.\n#\n# If the above options are not used, the normal Redis Cluster auto-detection\n# will be used instead.\n#\n# Note that when remapped, the bus port may not be at the fixed offset of\n# clients port + 10000, so you can specify any port and bus-port depending\n# on how they get remapped. If the bus-port is not set, a fixed offset of\n# 10000 will be used as usually.\n#\n# Example:\n#\n# cluster-announce-ip 10.1.1.5\n# cluster-announce-port 6379\n# cluster-announce-bus-port 6380\n\n################################## SLOW LOG ###################################\n\n# The Redis Slow Log is a system to log queries that exceeded a specified\n# execution time. The execution time does not include the I/O operations\n# like talking with the client, sending the reply and so forth,\n# but just the time needed to actually execute the command (this is the only\n# stage of command execution where the thread is blocked and can not serve\n# other requests in the meantime).\n#\n# You can configure the slow log with two parameters: one tells Redis\n# what is the execution time, in microseconds, to exceed in order for the\n# command to get logged, and the other parameter is the length of the\n# slow log. When a new command is logged the oldest one is removed from the\n# queue of logged commands.\n\n# The following time is expressed in microseconds, so 1000000 is equivalent\n# to one second. Note that a negative number disables the slow log, while\n# a value of zero forces the logging of every command.\nslowlog-log-slower-than 10000\n\n# There is no limit to this length. Just be aware that it will consume memory.\n# You can reclaim memory used by the slow log with SLOWLOG RESET.\nslowlog-max-len 128\n\n################################ LATENCY MONITOR ##############################\n\n# The Redis latency monitoring subsystem samples different operations\n# at runtime in order to collect data related to possible sources of\n# latency of a Redis instance.\n#\n# Via the LATENCY command this information is available to the user that can\n# print graphs and obtain reports.\n#\n# The system only logs operations that were performed in a time equal or\n# greater than the amount of milliseconds specified via the\n# latency-monitor-threshold configuration directive. When its value is set\n# to zero, the latency monitor is turned off.\n#\n# By default latency monitoring is disabled since it is mostly not needed\n# if you don&#39;t have latency issues, and collecting data has a performance\n# impact, that while very small, can be measured under big load. Latency\n# monitoring can easily be enabled at runtime using the command\n# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.\nlatency-monitor-threshold 0\n\n############################# EVENT NOTIFICATION ##############################\n\n# Redis can notify Pub/Sub clients about events happening in the key space.\n# This feature is documented at http://redis.io/topics/notifications\n#\n# For instance if keyspace events notification is enabled, and a client\n# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two\n# messages will be published via Pub/Sub:\n#\n# PUBLISH __keyspace@0__:foo del\n# PUBLISH __keyevent@0__:del foo\n#\n# It is possible to select the events that Redis will notify among a set\n# of classes. Every class is identified by a single character:\n#\n#  K     Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.\n#  E     Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.\n#  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...\n#  $     String commands\n#  l     List commands\n#  s     Set commands\n#  h     Hash commands\n#  z     Sorted set commands\n#  x     Expired events (events generated every time a key expires)\n#  e     Evicted events (events generated when a key is evicted for maxmemory)\n#  A     Alias for g$lshzxe, so that the &quot;AKE&quot; string means all the events.\n#\n#  The &quot;notify-keyspace-events&quot; takes as argument a string that is composed\n#  of zero or multiple characters. The empty string means that notifications\n#  are disabled.\n#\n#  Example: to enable list and generic events, from the point of view of the\n#           event name, use:\n#\n#  notify-keyspace-events Elg\n#\n#  Example 2: to get the stream of the expired keys subscribing to channel\n#             name __keyevent@0__:expired use:\n#\n#  notify-keyspace-events Ex\n#\n#  By default all notifications are disabled because most users don&#39;t need\n#  this feature and the feature has some overhead. Note that if you don&#39;t\n#  specify at least one of K or E, no events will be delivered.\nnotify-keyspace-events &quot;&quot;\n\n############################### ADVANCED CONFIG ###############################\n\n# Hashes are encoded using a memory efficient data structure when they have a\n# small number of entries, and the biggest entry does not exceed a given\n# threshold. These thresholds can be configured using the following directives.\nhash-max-ziplist-entries 512\nhash-max-ziplist-value 64\n\n# Lists are also encoded in a special way to save a lot of space.\n# The number of entries allowed per internal list node can be specified\n# as a fixed maximum size or a maximum number of elements.\n# For a fixed maximum size, use -5 through -1, meaning:\n# -5: max size: 64 Kb  &lt;-- not recommended for normal workloads\n# -4: max size: 32 Kb  &lt;-- not recommended\n# -3: max size: 16 Kb  &lt;-- probably not recommended\n# -2: max size: 8 Kb   &lt;-- good\n# -1: max size: 4 Kb   &lt;-- good\n# Positive numbers mean store up to _exactly_ that number of elements\n# per list node.\n# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),\n# but if your use case is unique, adjust the settings as necessary.\nlist-max-ziplist-size -2\n\n# Lists may also be compressed.\n# Compress depth is the number of quicklist ziplist nodes from *each* side of\n# the list to *exclude* from compression.  The head and tail of the list\n# are always uncompressed for fast push/pop operations.  Settings are:\n# 0: disable all list compression\n# 1: depth 1 means &quot;don&#39;t start compressing until after 1 node into the list,\n#    going from either the head or tail&quot;\n#    So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]\n#    [head], [tail] will always be uncompressed; inner nodes will compress.\n# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]\n#    2 here means: don&#39;t compress head or head-&gt;next or tail-&gt;prev or tail,\n#    but compress all nodes between them.\n# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]\n# etc.\nlist-compress-depth 0\n\n# Sets have a special encoding in just one case: when a set is composed\n# of just strings that happen to be integers in radix 10 in the range\n# of 64 bit signed integers.\n# The following configuration setting sets the limit in the size of the\n# set in order to use this special memory saving encoding.\nset-max-intset-entries 512\n\n# Similarly to hashes and lists, sorted sets are also specially encoded in\n# order to save a lot of space. This encoding is only used when the length and\n# elements of a sorted set are below the following limits:\nzset-max-ziplist-entries 128\nzset-max-ziplist-value 64\n\n# HyperLogLog sparse representation bytes limit. The limit includes the\n# 16 bytes header. When an HyperLogLog using the sparse representation crosses\n# this limit, it is converted into the dense representation.\n#\n# A value greater than 16000 is totally useless, since at that point the\n# dense representation is more memory efficient.\n#\n# The suggested value is ~ 3000 in order to have the benefits of\n# the space efficient encoding without slowing down too much PFADD,\n# which is O(N) with the sparse encoding. The value can be raised to\n# ~ 10000 when CPU is not a concern, but space is, and the data set is\n# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.\nhll-sparse-max-bytes 3000\n\n# Streams macro node max size / items. The stream data structure is a radix\n# tree of big nodes that encode multiple items inside. Using this configuration\n# it is possible to configure how big a single node can be in bytes, and the\n# maximum number of items it may contain before switching to a new node when\n# appending new stream entries. If any of the following settings are set to\n# zero, the limit is ignored, so for instance it is possible to set just a\n# max entires limit by setting max-bytes to 0 and max-entries to the desired\n# value.\nstream-node-max-bytes 4096\nstream-node-max-entries 100\n\n# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in\n# order to help rehashing the main Redis hash table (the one mapping top-level\n# keys to values). The hash table implementation Redis uses (see dict.c)\n# performs a lazy rehashing: the more operation you run into a hash table\n# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the\n# server is idle the rehashing is never complete and some more memory is used\n# by the hash table.\n#\n# The default is to use this millisecond 10 times every second in order to\n# actively rehash the main dictionaries, freeing memory when possible.\n#\n# If unsure:\n# use &quot;activerehashing no&quot; if you have hard latency requirements and it is\n# not a good thing in your environment that Redis can reply from time to time\n# to queries with 2 milliseconds delay.\n#\n# use &quot;activerehashing yes&quot; if you don&#39;t have such hard requirements but\n# want to free memory asap when possible.\nactiverehashing yes\n\n# The client output buffer limits can be used to force disconnection of clients\n# that are not reading data from the server fast enough for some reason (a\n# common reason is that a Pub/Sub client can&#39;t consume messages as fast as the\n# publisher can produce them).\n#\n# The limit can be set differently for the three different classes of clients:\n#\n# normal -&gt; normal clients including MONITOR clients\n# replica  -&gt; replica clients\n# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern\n#\n# The syntax of every client-output-buffer-limit directive is the following:\n#\n# client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;\n#\n# A client is immediately disconnected once the hard limit is reached, or if\n# the soft limit is reached and remains reached for the specified number of\n# seconds (continuously).\n# So for instance if the hard limit is 32 megabytes and the soft limit is\n# 16 megabytes / 10 seconds, the client will get disconnected immediately\n# if the size of the output buffers reach 32 megabytes, but will also get\n# disconnected if the client reaches 16 megabytes and continuously overcomes\n# the limit for 10 seconds.\n#\n# By default normal clients are not limited because they don&#39;t receive data\n# without asking (in a push way), but just after a request, so only\n# asynchronous clients may create a scenario where data is requested faster\n# than it can read.\n#\n# Instead there is a default limit for pubsub and replica clients, since\n# subscribers and replicas receive data in a push fashion.\n#\n# Both the hard or the soft limit can be disabled by setting them to zero.\nclient-output-buffer-limit normal 0 0 0\nclient-output-buffer-limit replica 256mb 64mb 60\nclient-output-buffer-limit pubsub 32mb 8mb 60\n\n# Client query buffers accumulate new commands. They are limited to a fixed\n# amount by default in order to avoid that a protocol desynchronization (for\n# instance due to a bug in the client) will lead to unbound memory usage in\n# the query buffer. However you can configure it here if you have very special\n# needs, such us huge multi/exec requests or alike.\n#\n# client-query-buffer-limit 1gb\n\n# In the Redis protocol, bulk requests, that are, elements representing single\n# strings, are normally limited ot 512 mb. However you can change this limit\n# here.\n#\n# proto-max-bulk-len 512mb\n\n# Redis calls an internal function to perform many background tasks, like\n# closing connections of clients in timeout, purging expired keys that are\n# never requested, and so forth.\n#\n# Not all tasks are performed with the same frequency, but Redis checks for\n# tasks to perform according to the specified &quot;hz&quot; value.\n#\n# By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when\n# Redis is idle, but at the same time will make Redis more responsive when\n# there are many keys expiring at the same time, and timeouts may be\n# handled with more precision.\n#\n# The range is between 1 and 500, however a value over 100 is usually not\n# a good idea. Most users should use the default of 10 and raise this up to\n# 100 only in environments where very low latency is required.\nhz 10\n\n# Normally it is useful to have an HZ value which is proportional to the\n# number of clients connected. This is useful in order, for instance, to\n# avoid too many clients are processed for each background task invocation\n# in order to avoid latency spikes.\n#\n# Since the default HZ value by default is conservatively set to 10, Redis\n# offers, and enables by default, the ability to use an adaptive HZ value\n# which will temporary raise when there are many connected clients.\n#\n# When dynamic HZ is enabled, the actual configured HZ will be used as\n# as a baseline, but multiples of the configured HZ value will be actually\n# used as needed once more clients are connected. In this way an idle\n# instance will use very little CPU time while a busy instance will be\n# more responsive.\ndynamic-hz yes\n\n# When a child rewrites the AOF file, if the following option is enabled\n# the file will be fsync-ed every 32 MB of data generated. This is useful\n# in order to commit the file to the disk more incrementally and avoid\n# big latency spikes.\naof-rewrite-incremental-fsync yes\n\n# When redis saves RDB file, if the following option is enabled\n# the file will be fsync-ed every 32 MB of data generated. This is useful\n# in order to commit the file to the disk more incrementally and avoid\n# big latency spikes.\nrdb-save-incremental-fsync yes\n\n# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good\n# idea to start with the default settings and only change them after investigating\n# how to improve the performances and how the keys LFU change over time, which\n# is possible to inspect via the OBJECT FREQ command.\n#\n# There are two tunable parameters in the Redis LFU implementation: the\n# counter logarithm factor and the counter decay time. It is important to\n# understand what the two parameters mean before changing them.\n#\n# The LFU counter is just 8 bits per key, it&#39;s maximum value is 255, so Redis\n# uses a probabilistic increment with logarithmic behavior. Given the value\n# of the old counter, when a key is accessed, the counter is incremented in\n# this way:\n#\n# 1. A random number R between 0 and 1 is extracted.\n# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).\n# 3. The counter is incremented only if R &lt; P.\n#\n# The default lfu-log-factor is 10. This is a table of how the frequency\n# counter changes with a different number of accesses with different\n# logarithmic factors:\n#\n# +--------+------------+------------+------------+------------+------------+\n# | factor | 100 hits   | 1000 hits  | 100K hits  | 1M hits    | 10M hits   |\n# +--------+------------+------------+------------+------------+------------+\n# | 0      | 104        | 255        | 255        | 255        | 255        |\n# +--------+------------+------------+------------+------------+------------+\n# | 1      | 18         | 49         | 255        | 255        | 255        |\n# +--------+------------+------------+------------+------------+------------+\n# | 10     | 10         | 18         | 142        | 255        | 255        |\n# +--------+------------+------------+------------+------------+------------+\n# | 100    | 8          | 11         | 49         | 143        | 255        |\n# +--------+------------+------------+------------+------------+------------+\n#\n# NOTE: The above table was obtained by running the following commands:\n#\n#   redis-benchmark -n 1000000 incr foo\n#   redis-cli object freq foo\n#\n# NOTE 2: The counter initial value is 5 in order to give new objects a chance\n# to accumulate hits.\n#\n# The counter decay time is the time, in minutes, that must elapse in order\n# for the key counter to be divided by two (or decremented if it has a value\n# less &lt;= 10).\n#\n# The default value for the lfu-decay-time is 1. A Special value of 0 means to\n# decay the counter every time it happens to be scanned.\n#\n# lfu-log-factor 10\n# lfu-decay-time 1\n\n########################### ACTIVE DEFRAGMENTATION #######################\n#\n# WARNING THIS FEATURE IS EXPERIMENTAL. However it was stress tested\n# even in production and manually tested by multiple engineers for some\n# time.\n#\n# What is active defragmentation?\n# -------------------------------\n#\n# Active (online) defragmentation allows a Redis server to compact the\n# spaces left between small allocations and deallocations of data in memory,\n# thus allowing to reclaim back memory.\n#\n# Fragmentation is a natural process that happens with every allocator (but\n# less so with Jemalloc, fortunately) and certain workloads. Normally a server\n# restart is needed in order to lower the fragmentation, or at least to flush\n# away all the data and create it again. However thanks to this feature\n# implemented by Oran Agra for Redis 4.0 this process can happen at runtime\n# in an &quot;hot&quot; way, while the server is running.\n#\n# Basically when the fragmentation is over a certain level (see the\n# configuration options below) Redis will start to create new copies of the\n# values in contiguous memory regions by exploiting certain specific Jemalloc\n# features (in order to understand if an allocation is causing fragmentation\n# and to allocate it in a better place), and at the same time, will release the\n# old copies of the data. This process, repeated incrementally for all the keys\n# will cause the fragmentation to drop back to normal values.\n#\n# Important things to understand:\n#\n# 1. This feature is disabled by default, and only works if you compiled Redis\n#    to use the copy of Jemalloc we ship with the source code of Redis.\n#    This is the default with Linux builds.\n#\n# 2. You never need to enable this feature if you don&#39;t have fragmentation\n#    issues.\n#\n# 3. Once you experience fragmentation, you can enable this feature when\n#    needed with the command &quot;CONFIG SET activedefrag yes&quot;.\n#\n# The configuration parameters are able to fine tune the behavior of the\n# defragmentation process. If you are not sure about what they mean it is\n# a good idea to leave the defaults untouched.\n\n# Enabled active defragmentation\n# activedefrag yes\n\n# Minimum amount of fragmentation waste to start active defrag\n# active-defrag-ignore-bytes 100mb\n\n# Minimum percentage of fragmentation to start active defrag\n# active-defrag-threshold-lower 10\n\n# Maximum percentage of fragmentation at which we use maximum effort\n# active-defrag-threshold-upper 100\n\n# Minimal effort for defrag in CPU percentage\n# active-defrag-cycle-min 5\n\n# Maximal effort for defrag in CPU percentage\n# active-defrag-cycle-max 75\n\n# Maximum number of set/hash/zset/list fields that will be processed from\n# the main dictionary scan\n# active-defrag-max-scan-fields 1000\n\n启动命令bashdocker run -p 6379:6379 \\\n-v /Users/oak/docker/redis/data:/data \\\n-v /Users/oak/docker/redis/conf/redis.conf:/usr/local/etc/redis/redis.conf \\\n--privileged=true  \\\n--name redis -d redis:5.0.14 \\\nredis-server  /usr/local/etc/redis/redis.conf","slug":"Docker-搭建-Redis","date":"2018-04-04T03:51:36.000Z","categories_index":"Docker,Redis","tags_index":"Docker,Redis","author_index":"GlassCat"},{"id":"4a8fd3b619d191ce486290c388b0880b","title":"Docker 搭建 MySQL","content":"创建宿主机目录bashmkdir -p /Users/oak/docker/mysql/config \nmkdir -p /Users/oak/docker/mysql/data \nmkdir -p /Users/oak/docker/mysql/log准备配置文件在 /Users/oak/docker/mysql/config 目录下创建 custom.conf 文件\nMySQL 5.7.Xshell[mysqld]\ndefault-time-zone = &#39;+8:00&#39;\nlog_timestamps = SYSTEM\nskip-name-resolve\nlog-bin\nserver_id=573417\ncharacter_set_server=utf8mb4\nmax_connections=300MySQL 8.0.Xshell[mysqld]\ndefault-time-zone = &#39;+8:00&#39;\nlog_timestamps = SYSTEM\nskip-name-resolve\nlog-bin\nserver_id=803418\ncharacter_set_server=utf8mb4\n# 数据库字符集对应一些排序等规则\n# collation-server=utf8mb4_general_ci\n# 设置client连接mysql时的字符集,防止乱码\n# init_connect=&#39;SET NAMES utf8mb4&#39;\ndefault_authentication_plugin=mysql_native_password\nmax_connections=300MySQL 8.4.Xshell[mysqld]\ndefault-time-zone = &#39;+8:00&#39;\nlog_timestamps = SYSTEM\nskip-name-resolve\nlog-bin\nserver_id=843419\ncharacter_set_server=utf8mb4\nmysql_native_password=on\nmax_connections=300启动命令MySQL 5.7.Xshelldocker run -h mysql -p 3306:3306 \\\n--restart always \\\n--privileged=true \\\n--name mysql \\\n-e MYSQL_ROOT_PASSWORD=123456 \\\n-e MYSQL_DATABASE=ecs \\\n-e TZ=Asia/Shanghai \\\n-v /Users/oak/docker/mysql/config:/etc/mysql/conf.d \\\n-v /Users/oak/docker/mysql/data:/var/lib/mysql \\\n-v /Users/oak/docker/mysql/log:/logs \\\n-d mysql:5.7.44\nMySQL 8.0.Xbash\ndocker run -d -p 3306:3306 \\\n--restart always \\\n--privileged=true \\\n--name mysql \\\n-e MYSQL_ROOT_PASSWORD=123456 \\\n-e TZ=Asia/Shanghai \\\n-e MYSQL_DATABASE=ecs \\\n-v /Users/oak/docker/mysql/config:/etc/mysql/conf.d \\\n-v /Users/oak/docker/mysql/data:/var/lib/mysql \\\n-v /Users/oak/docker/mysql/log:/logs \\\nmysql:8.0 \\\n--character-set-server=utf8mb4 \\\n--collation-server=utf8mb4_general_ci \\\n--default-time_zone=&#39;+8:00&#39;bash参数说明：\nMYSQL_ROOT_PASSWORD ： 设置mysql数据库root的密码\nMYSQL_DATABASE ： 启动时创建数据库\nTZ=Asia/shanghai ： 设置容器时区\ncharacter-set-server ： 服务器字符集，在创建数据库和表时不特别指定字符集，这样统一采用character-set-server字符集。\ncharacter-set-database ： 数据库字符集\ncharacter-set-table ： 数据库表字符集\ncollation-server ： 排序规则字符集\ndefault-time_zone ： mysql的时区，如果命令行不设置，且配置文件中也没有配置，则继承容器的时区MySQL 8.4.Xbashdocker run -h mysql -p 3306:3306 \\\n--restart always \\\n--privileged=true \\\n--name mysql \\\n-e MYSQL_ROOT_PASSWORD=123456 \\\n-e MYSQL_DATABASE=ecs \\\n-e TZ=Asia/Shanghai \\\n-v /Users/oak/docker/mysql/config:/etc/mysql/conf.d \\\n-v /Users/oak/docker/mysql/data:/var/lib/mysql \\\n-v /Users/oak/docker/mysql/log:/logs \\\n-d mysql:8.4.5\n","slug":"Docker-搭建-MySQL","date":"2018-04-04T03:49:36.000Z","categories_index":"Docker,MySQL","tags_index":"Docker,MySQL","author_index":"GlassCat"},{"id":"f19f5567a9ee1ed08029743c3538f587","title":"MacBook 多版本JDK管理","content":"Mac OS 的 JDK安装完后的路径：\n\nMacOS 提供的在 &#x2F;System&#x2F;Library&#x2F;Java&#x2F;JavaVirtualMachines\nOracle 提供的在 &#x2F;Library&#x2F;Java&#x2F;JavaVirtualMachines\n\n这些路径都可以通过一个 java_home 的命令行来取得\n查看已安装的JDK使用 java_home -V 查看本机所有 JDK\nbash☁  ~  /usr/libexec/java_home -V\nMatching Java Virtual Machines (5):\n    23.0.2 (x86_64) &quot;Oracle Corporation&quot; - &quot;OpenJDK 23.0.2&quot; /Users/oak/Library/Java/JavaVirtualMachines/openjdk-23.0.2/Contents/Home\n    21.0.1 (x86_64) &quot;Oracle Corporation&quot; - &quot;OpenJDK 21.0.1&quot; /Users/oak/Library/Java/JavaVirtualMachines/openjdk-21.0.1/Contents/Home\n    17.0.2 (x86_64) &quot;Oracle Corporation&quot; - &quot;OpenJDK 17.0.2&quot; /Users/oak/Library/Java/JavaVirtualMachines/openjdk-17.0.2/Contents/Home\n    11.0.25 (x86_64) &quot;Amazon.com Inc.&quot; - &quot;Amazon Corretto 11&quot; /Users/oak/Library/Java/JavaVirtualMachines/corretto-11.0.25/Contents/Home\n    1.8.0_452 (x86_64) &quot;Amazon&quot; - &quot;Amazon Corretto 8&quot; /Users/oak/Library/Java/JavaVirtualMachines/corretto-1.8.0_452/Contents/Home\n/Users/oak/Library/Java/JavaVirtualMachines/openjdk-23.0.2/Contents/Home使用 java_home -v 11 查看指定版本的JDK\nbash☁  ~  /usr/libexec/java_home -v 11\n/Users/oak/Library/Java/JavaVirtualMachines/corretto-11.0.25/Contents/Home\n☁  ~  /usr/libexec/java_home -v 17\n/Users/oak/Library/Java/JavaVirtualMachines/openjdk-17.0.2/Contents/Home\n☁  ~  /usr/libexec/java_home -v 21\n/Users/oak/Library/Java/JavaVirtualMachines/openjdk-21.0.1/Contents/Home\n☁  ~  /usr/libexec/java_home -v 23\n/Users/oak/Library/Java/JavaVirtualMachines/openjdk-23.0.2/Contents/Home配置JAVA_HOME取到了路径，直接赋值给 JAVA_HOME ，打开 .bash_profile 或者 .zshrc ，用 alias 起一个别名\n我的是 .zshrc 如下\nbash# Example aliases\n# alias zshconfig=&quot;mate ~/.zshrc&quot;\n# alias ohmyzsh=&quot;mate ~/.oh-my-zsh\nalias jdk8=&#39;export JAVA_HOME=`/usr/libexec/java_home -v 1.8`&#39;\nalias jdk11=&#39;export JAVA_HOME=`/usr/libexec/java_home -v 11`&#39;\nalias jdk17=&#39;export JAVA_HOME=`/usr/libexec/java_home -v 17`&#39;\nalias jdk21=&#39;export JAVA_HOME=`/usr/libexec/java_home -v 21`&#39;\nalias jdk23=&#39;export JAVA_HOME=`/usr/libexec/java_home -v 23`&#39;然后在命令行输入 source .zshrc 让配置生效\nbash☁  ~  source .zshrc切换JDKbash☁  ~  jdk17\n☁  ~  java -version\nopenjdk version &quot;17.0.2&quot; 2022-01-18\nOpenJDK Runtime Environment (build 17.0.2+8-86)\nOpenJDK 64-Bit Server VM (build 17.0.2+8-86, mixed mode, sharing)\n☁  ~  jdk11\n☁  ~  java -version\nopenjdk version &quot;11.0.25&quot; 2024-10-15 LTS\nOpenJDK Runtime Environment Corretto-11.0.25.9.1 (build 11.0.25+9-LTS)\nOpenJDK 64-Bit Server VM Corretto-11.0.25.9.1 (build 11.0.25+9-LTS, mixed mode)\n☁  ~  jdk8\n☁  ~  java -version\nopenjdk version &quot;1.8.0_452&quot;\nOpenJDK Runtime Environment Corretto-8.452.09.1 (build 1.8.0_452-b09)\nOpenJDK 64-Bit Server VM Corretto-8.452.09.1 (build 25.452-b09, mixed mode)","slug":"MacBook多版本JDK管理","date":"2018-03-31T05:26:42.000Z","categories_index":"MacBook,环境搭建","tags_index":"MacBook,环境搭建,JDK","author_index":"GlassCat"},{"id":"ebab0b3cf4c39bee5f017078a6c91559","title":"MacBook 后端环境搭建","content":"\n\n\n\n\n\n\n\n\n为了减少在安装过程中少受折磨，配置环境之前，尽量先打开科学上网，不然你可能会和我一样口吐芬芳\niTerm2 终端用 iTerm2 + zsh + zsh-syntax-highlighting 打造好用又美观的终端工具\n参考文章🔗\n\nMac下iTerm2+oh my zsh+powerlevel10k 配置与美化过程记录 \n这篇 iTerm2 + Oh My Zsh 教程手把手让你成为这条街最靓的仔\n\n先安静地完整浏览一遍，再进行实操，会少走很多弯路\nHomeBrewhomebrew 是 MacOS 上非常流行的开源包管理器，可以理解为一个命令行版本的应用商店\n讲真的，Homebrew 术语有点羞涩难懂，本身有自制酿酒之意，诸如 Formula、Cask 等也是与酿酒相关的\n参考文章🔗\n\nHomebrew 使用详解\n\n先安静地完整浏览一遍，再进行实操，会少走很多弯路\nGit 版本控制Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目\n可以直接通过 homebrew 安装 \nbashbrew install git参考文章🔗\n\n使用Homebrew安装Git\n\n先安静地完整浏览一遍，再进行实操，会少走很多弯路\nJetBrains全家桶IDEA + DataSpell\n\nIDEA: Java后端开发神器\nDataSpell: 大数据开发神器，支持各种 RDB、NoSQL 缓存数据库、文档数据库、CSV、Excel等，各种听过的没听过的都支持，可以使用 Python、R 语言、Jupyter Notebook 来处理数据\n\n不过JetBrains对于个人开发者来说，是十分昂贵的，只能参考某些体验方案\n参考文章🔗\n\nJetBrains全家桶完美体验方案\n\n先安静地完整浏览一遍，再进行实操，会少走很多弯路\nIDEA 插件\n\n\n名称\n作用\n\n\n\n.gitignore\ngitignore 工具\n\n\nAlibaba Java Coding Guidelines\nAlibaba  Java 开发规范\n\n\nCharAutoReplace\n中文字符自动替换成英文字符插件。提高了写代码效率，增加了键盘寿命。再也不用担心写注释和代码忘记切换输入法了\n\n\nEasyCode\n基于IntelliJ IDEA开发的代码生成插件，支持自定义任意模板（Java，html，js，xml）\n\n\nGitToolBox\n强大美观的 git 插件\n\n\nGradianto\n主题插件，钟爱 middle blue\n\n\nGrep Console\n控制台插件，美化高亮日志输出\n\n\nMaven Helper\nMaven 可视化插件，树形展示 Maven 依赖\n\n\nMapStruct Support\n这个插件为使用MapStruct生成Bean映射代码的项目提供了一些帮助\n\n\nRainbow Brackets\n彩虹括号是一个文本编辑器插件，用于帮助程序员识别代码中括号（包括圆括号、方括号和大括号）的嵌套层次\n\n\nTranslation\n翻译插件，特别好用\n\n\nMyBatisCodeHelperPro\nMyBatisCodeHelperPro 是一款专为Mybatis框架设计的代码生成工具，旨在提高开发效率，减少重复劳动。 它能够自动生成常见的Mapper接口、XML配置文件、Service接口与实现类以及DAO接口与实现类，大大简化了基于Mybatis.\n\n\nSonarQube for IDE\nSonar 的 SonarQube for IDE（以前称为 SonarLint）是一个免费的 IDE 扩展，可以实时查找和修复代码问题，在您编写代码时标记问题，就像拼写检查器一样。它不仅仅是一个代码检查工具，还提供了丰富的上下文指导，帮助开发人员理解为什么会出现问题、评估风险并教他们如何修复问题。这有助于提高他们的技能，增强他们的生产力，并让他们对自己的代码负责，将代码检查提升到了一个新的水平。\n\n\nString Manipulate\n字符串转换处理插件，很强大的转换功能\n\n\nResource Bundle Editor\n它是一个用于编辑 i18n 的属性文件， 它让你通一个屏幕就能够同时管理所有相关联属性文件中的key&#x2F;value信息\n\n\nRestfulTool\nRestfulTool 是一套 Restful 服务开发辅助工具集\n\n\nApache Dubbo in Spring Framework\n增加了对在Spring框架中使用Apache Dubbo框架的支持\n\n\nEncoding 设置统一 UTF8 编码\n\n字体设置只设置编辑器的字体和字号就好，其它地方还是保持默认就 OK\n\nJava环境配置我建议直接打开IDEA去安装JDK\n可以安装多个版本JDK，然后用多版本管理 MacBook多版本JDK管理\nVS Code 编辑器VS Code 可以说是万能的代码编辑器，依托于丰富的插件市场，可以与任意编程语言集成，不过对于 Java 来说，还是差点意思，更多的还是用于编写前端，对于后端来说，用来当做一个万能编辑器还是 very nice ！\nDocker Desktop轻量级的虚拟化技术，同时是一个开源的应用容器引擎，可以让开发者以便捷方式打包应用到可移植的容器中，然后安装至任何运行 Linux、Windows 等系统的服务器\n安装中间件中间件都用 Docker 安装, 避免污染本地环境, 而且通过 Docker Desktop 方便启停中间件\nMySQL参考我的文章🔗 Docker搭建MySQL\nRedis参考我的文章🔗 Docker搭建Redis\nZookeeper参考我的文章🔗 Docker搭建Zookeeper\nNacos参考我的文章🔗 Docker搭建Nacos\nElasticSearch + Kibana参考我的文章🔗 Docker搭建ElasticSearch+Kibana\n","slug":"MacBook-后端环境搭建","date":"2018-03-31T03:36:42.000Z","categories_index":"MacBook,环境搭建","tags_index":"MacBook,环境搭建","author_index":"GlassCat"},{"id":"78eda1e73784e2bed9dff22f7b33de09","title":"分布式ID设计方案","content":"\n\n\n\n\n\n\n\n\n是的，之前部门布置的任务完成的不错，老大继续给我分配了一个设计分布式ID方案的任务，不说了，加班加班~\n在一个分布式系统中，如何保证订单ID的全局唯一性？\n核心要点针对这个问题，我会从以下几点考虑：\n1、可以使用UUID&#x2F;GUID方案，它能保证全球范围内的唯一性，无需中心化协调，但缺点是长度较长且无序。\n2、雪花算法(Snowflake)是一个很好的选择，它由时间戳+机器ID+序列号组成，既保证了唯一性又保证了有序性，非常适合订单系统。\n3、对于大型系统，我们可以考虑号段模式，即集中式ID生成器预分配一段ID给各服务，减少网络请求。\n4、还有Redis自增和数据库自增策略等也是常见方案，但需注意高可用问题。\n实际项目中，我会根据具体的业务需求、数据量和性能要求选择最合适的方案，通常雪花算法是最均衡的选择\n详细解析全局唯一ID的核心需求 🎯做好订单ID设计，我们需要关注这几点：\n\n全局唯一性：确保在整个分布式系统中不会出现重复 ID\n\n高性能：生成过程要快，不能成为系统瓶颈\n\n高可用：ID 生成器不能有单点故障\n\n趋势递增：对于订单系统，通常希望 ID 保持递增趋势，便于排序和分析\n\n\n常见解决方案分析 🛠️1. UUID&#x2F;GUID 📌javaString uuid = UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;);UUID是最简单直接的方案，但存在几个问题：\n\n长度过长，16字节\n\n无序性导致索引效率低下\n\n用户体验不友好，不容易记忆和沟通\n\n\n2. 雪花算法(Snowflake) ❄️雪花算法是Twitter开源的分布式ID生成算法，核心思想是：\n\n1位符号位，固定为0\n\n41位时间戳，精确到毫秒\n\n10位机器ID，可以部署在1024个节点\n\n12位序列号，同一毫秒内可生成4096个ID\n\n\n优势非常明显：\n\n趋势自增，满足订单排序需求\n\n生成效率高，不依赖第三方存储\n\nID长度适中\n\n可以根据ID反解析出时间和机器信息，便于问题排查\n\n\n实现示例：\njavapublic class SnowflakeIdGenerator &#123;\n    private final long workerId;\n    private final long datacenterId;\n    private long sequence = 0L;\n    \n    // 时间起点 (2017-01-01)\n    private final long twepoch = 1483200000000L;\n    \n    // 机器ID位数，数据中心ID位数，序列号位数\n    private final long workerIdBits = 5L;\n    private final long datacenterIdBits = 5L;\n    private final long sequenceBits = 12L;\n    \n    // 各部分最大值\n    private final long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits);\n    private final long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits);\n    private final long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits);\n    \n    // 各部分偏移量\n    private final long workerIdShift = sequenceBits;\n    private final long datacenterIdShift = sequenceBits + workerIdBits;\n    private final long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;\n    \n    private long lastTimestamp = -1L;\n    \n    // 构造函数\n    public SnowflakeIdGenerator(long workerId, long datacenterId) &#123;\n        if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123;\n            throw new IllegalArgumentException(&quot;Worker ID can&#39;t be greater than &quot; + maxWorkerId + &quot; or less than 0&quot;);\n        &#125;\n        if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123;\n            throw new IllegalArgumentException(&quot;Datacenter ID can&#39;t be greater than &quot; + maxDatacenterId + &quot; or less than 0&quot;);\n        &#125;\n        this.workerId = workerId;\n        this.datacenterId = datacenterId;\n    &#125;\n    \n    // 生成下一个ID\n    public synchronized long nextId() &#123;\n        long timestamp = timeGen();\n        if (timestamp &lt; lastTimestamp) &#123;\n            throw new RuntimeException(&quot;Clock moved backwards. Refusing to generate id&quot;);\n        &#125;\n        if (lastTimestamp == timestamp) &#123;\n            sequence = (sequence + 1) &amp; sequenceMask;\n            if (sequence == 0) &#123;\n                timestamp = tilNextMillis(lastTimestamp);\n            &#125;\n        &#125; else &#123;\n            sequence = 0L;\n        &#125;\n        lastTimestamp = timestamp;\n        return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) |\n                (datacenterId &lt;&lt; datacenterIdShift) |\n                (workerId &lt;&lt; workerIdShift) |\n                sequence;\n    &#125;\n    \n    private long tilNextMillis(long lastTimestamp) &#123;\n        long timestamp = timeGen();\n        while (timestamp &lt;= lastTimestamp) &#123;\n            timestamp = timeGen();\n        &#125;\n        return timestamp;\n    &#125;\n    \n    private long timeGen() &#123;\n        return System.currentTimeMillis();\n    &#125;\n&#125;3. 号段模式 🔢号段模式是一种优化的中心化ID分配策略，核心思想是：\n\n中心服务一次性分配一段ID给应用服务器\n\n应用服务器在内存中分配这段ID，用完再去申请\n\n减少了频繁网络请求，提高了性能\n\n\n\n4. 其他方案比较 📊\n\n\n方案\n优点\n缺点\n适用场景\n\n\n\n数据库自增ID\n简单易实现，有序\n单点故障风险，性能瓶颈\n小型系统，数据量不大\n\n\nRedis自增\n性能较好，实现简单\n需要考虑持久化，可靠性问题\n中型系统，对性能有一定要求\n\n\n雪花算法\n高性能，无单点，趋势递增\n需要解决时钟回退问题\n大型分布式系统\n\n\nUUID\n简单，无需中心化服务\n无序，长度长，索引效率低\n对ID格式无特殊要求的场景\n\n\n号段模式\n减少网络请求，高性能\n实现稍复杂，需要集中管理\n大型系统，对性能要求高\n\n\n实战建议 🚀在项目中，我们选择了雪花算法作为订单ID生成方案，并做了以下改进：\n\n解决时钟回拨问题 - 加入了时钟回拨检测机制，如果发现回拨，会等待一段时间或报警\n机器ID分配 - 使用Zookeeper自动分配机器ID，避免手动配置\n每秒可生成409.6万个ID，完全满足高并发需求\n\n当你遇到这个问题时，我建议：\n\n分析你的业务需求：ID是否需要有序？数据规模有多大？\n评估系统的可用性要求和性能需求\n中小型系统可以考虑Redis+Lua或改进的雪花算法\n大型系统建议使用雪花算法或号段模式\n\n核心是，不要过度设计，选择满足需求的最简方案，同时预留扩展空间，这样才能既满足业务需求，又能保证系统的健壮性和可维护性。\n","slug":"分布式ID设计方案","date":"2017-08-25T05:04:29.000Z","categories_index":"分布式","tags_index":"分布式,分布式ID","author_index":"GlassCat"},{"id":"109296d43a046ab98a4514702e34310a","title":"分布式锁的三种实现方案","content":"\n\n\n\n\n\n\n\n\n部门最近给我分配了个任务，让我调研一下分布式锁的方案，然后做成组件\n作为一个刚毕业的菜鸟，我表示鸭梨三大，不过没什么是加班搞不定的，冲鸭 ~\n考虑在分布式系统中实现锁，通常需要保证互斥性、避免死锁、高可用等特性，Java 实现分布式锁的方案有三种：\n\n基于 Zookeeper 实现，使用 curator 实现\n基于 Redis 实现，手动实现，或者用 Redisson\n基于数据库 (MySQL) 实现，手动实现\n\n方案对比\n\n\n特性\n数据库方案\nRedis方案\nZooKeeper方案\n\n\n\n性能\n低 (500-1000 TPS)\n高 (10万+ TPS)\n中 (1万+ TPS)\n\n\n实现复杂度\n高\n低\n中\n\n\n可靠性\n依赖数据库稳定性\n依赖Redis集群\n依赖ZooKeeper集群\n\n\n锁释放机制\n需手动&#x2F;超时释放\n自动超时释放\n会话结束自动释放\n\n\n数据一致性\nCP 强一致\nAP 最终一致\nCP 强一致\n\n\n适用场景\n传统企业内部系统\n互联网高并发系统\n金融核心系统\n\n\n故障恢复\n主从切换可能丢锁\n半数节点存活即可服务\n主从切换可能丢锁\n\n\n建议：\n\n优先选择Redis方案（性能最佳）\n强一致性要求选ZooKeeper\n数据库方案不考虑\n\n方案实现基于Zookeeper核心机制解析锁节点结构bash[zk: 127.0.0.1:2181(CONNECTED) 23] ls /default/payment-lock/pay_id/1000\n[\n    _c_0c5367c6-f215-45b0-a4e5-db121fcb5fa3-lock-0000000037, \n    _c_18cad173-4a26-42ba-9bf6-dc3ea2c7c68f-lock-0000000032, \n    _c_1ad07601-fd65-4dea-bc4b-f43c50535f59-lock-0000000033, \n    _c_3073654e-21df-44e4-ad5d-e6465e656813-lock-0000000035, \n    _c_44c85913-6aef-490d-aaec-aa2f3f3c4e67-lock-0000000039, \n    _c_685522a8-af8e-4aed-bd71-ec45f37d52bc-lock-0000000038, \n    _c_68f9e57b-6b4f-4d43-b2a5-2b91b4474848-lock-0000000031, \n    _c_a21693b1-f6f6-4ff9-98a3-17b9c0057f47-lock-0000000034, \n    _c_bb91efd2-f696-4e98-b41b-3270828e0714-lock-0000000030, \n    _c_ec3a5b4d-f307-460d-8a97-6eb58f34f58d-lock-0000000036\n]释放锁时，会删除 _c_bb91efd2-f696-4e98-b41b-3270828e0714-lock-0000000030 这样的临时会话顺序节点，但是它们的父节点 &#x2F;pay_id&#x2F;1000 不会被删除。因此，高并发的业务场景下使用 zookeeper 分布式锁时，会留下很多的空节点，存在建立无用节点且多节点之间需要同步数据的问题\n加锁流程\n创建临时有序节点\n获取父节点下所有子节点\n判断当前节点是否序号最小\n若非最小，则监听前一个节点删除事件\n\n解锁流程\n删除自己创建的临时节点\n触发后续节点的Watcher通知\n\n工作流程\n实现方案Maven 依赖xml  &lt;properties&gt;\n    &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt;\n    &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt;\n    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\n    &lt;!--\n    Curator 5.0 支持zookeeper3.6.X，不再支持 zookeeper3.4.X\n    Curator 4.X 支持zookeeper3.5.X，软兼容3.4.X\n    Curator 2.X 支持zookeeper3.4.X\n    --&gt;\n    &lt;curator.version&gt;4.2.0&lt;/curator.version&gt;\n    &lt;zookeeper.version&gt;3.4.14&lt;/zookeeper.version&gt;\n  &lt;/properties&gt;\n\n  &lt;dependencies&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;\n      &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt;\n      &lt;version&gt;$&#123;curator.version&#125;&lt;/version&gt;\n      &lt;exclusions&gt;\n        &lt;exclusion&gt;\n          &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;\n          &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;\n        &lt;/exclusion&gt;\n      &lt;/exclusions&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n      &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt;\n      &lt;artifactId&gt;zookeeper&lt;/artifactId&gt;\n      &lt;version&gt;$&#123;zookeeper.version&#125;&lt;/version&gt;\n      &lt;scope&gt;compile&lt;/scope&gt;\n      &lt;exclusions&gt;\n        &lt;exclusion&gt;\n          &lt;groupId&gt;org.slf4j&lt;/groupId&gt;\n          &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;\n        &lt;/exclusion&gt;\n      &lt;/exclusions&gt;\n    &lt;/dependency&gt;\n  &lt;/dependencies&gt;配置类写一个 ZookeeperLockProperties 类，专门用于锁配置\njava@Data\n@ConfigurationProperties(prefix = &quot;lock.zookeeper&quot;)\npublic class ZookeeperLockProperties &#123;\n    private String addr = &quot;localhost:2181&quot;;\n    private String namespace = &quot;default&quot;;\n    private int sessionTimeout = 6000;\n    private int connectionTimeout = 3000;\n    private int maxRetries = 3;\n    private int retryInterval = 1000;\n    private int maxSleepTime = 1000;\n&#125;java@Configuration\n@EnableConfigurationProperties(ZookeeperLockProperties.class)\npublic class ZookeeperLockConfig &#123;\n\n    @Bean(initMethod = &quot;start&quot;, destroyMethod = &quot;close&quot;)\n    public CuratorFramework curatorFramework(ZookeeperLockProperties lockProperties) &#123;\n        RetryPolicy retryPolicy = new ExponentialBackoffRetry(lockProperties.getRetryInterval(),\n                lockProperties.getMaxRetries(), lockProperties.getMaxSleepTime());\n        return CuratorFrameworkFactory.builder()\n                .connectString(lockProperties.getAddr())\n                // 会话超时时间\n                .sessionTimeoutMs(lockProperties.getSessionTimeout())\n                // 连接超时时间\n                .connectionTimeoutMs(lockProperties.getConnectionTimeout())\n                .zk34CompatibilityMode(true)\n                .retryPolicy(retryPolicy)\n                // 命名空间隔离\n                .namespace(lockProperties.getNamespace())\n                .build();\n    &#125;\n&#125;分布式锁实现java\n@Component\n@RequiredArgsConstructor\npublic class ZookeeperLock &#123;\n\n    private final CuratorFramework curatorFramework;\n\n    /**\n     * 获取分布式锁（阻塞式）\n     *\n     * @param lockPath       锁路径（格式：/lock/resource_name）\n     * @param maxWaitSeconds 最大等待时间\n     */\n    public boolean acquireLock(String lockPath, int maxWaitSeconds) throws Exception &#123;\n        InterProcessMutex lock = new InterProcessMutex(curatorFramework, lockPath);\n        return lock.acquire(maxWaitSeconds, TimeUnit.SECONDS);\n    &#125;\n\n    /**\n     * 释放分布式锁\n     *\n     * @param lockPath 锁路径\n     */\n    public void releaseLock(String lockPath) throws Exception &#123;\n        InterProcessMutex lock = new InterProcessMutex(curatorFramework, lockPath);\n        if (lock.isAcquiredInThisProcess()) &#123;\n            lock.release();\n        &#125;\n    &#125;\n\n    /**\n     * 非阻塞尝试获取锁\n     *\n     * @return true=获取成功\n     */\n    public boolean tryLock(String lockPath) throws Exception &#123;\n        InterProcessMutex lock = new InterProcessMutex(curatorFramework, lockPath);\n        // 立即返回\n        return lock.acquire(0, null);\n    &#125;\n&#125;使用方式java@Service\n@RequiredArgsConstructor\npublic class Usage &#123;\n\n    private static final String PAYMENT_LOCK = &quot;/payment-lock/pay_id/&quot;;\n\n    private final ZookeeperLock lock;\n\n    public Boolean processPayment(String txId) &#123;\n        try &#123;\n            String lockKey = PAYMENT_LOCK + txId;\n            if (lock.acquireLock(lockKey, 5)) &#123;\n                // 核心处理逻辑（保证事务唯一性）\n                Thread.sleep(1000);\n                return Boolean.TRUE;\n            &#125;\n            return Boolean.FALSE;\n        &#125; catch (Exception e) &#123;\n            return Boolean.FALSE;\n        &#125; finally &#123;\n            try &#123;\n                lock.releaseLock(PAYMENT_LOCK);\n            &#125; catch (Exception ex) &#123;\n                // 记录日志\n            &#125;\n        &#125;\n    &#125;\n&#125;关键特性说明性能优化：\njava// 关闭不必要的Watcher\nInterProcessMutex lock = new InterProcessMutex(client, path, \n    new StandardLockInternalsDriver() &#123;\n        @Override\n        public PredicateResults getsTheLock(..., List&lt;String&gt; children) throws Exception &#123;\n            // 自定义获取锁逻辑\n        &#125;\n    &#125;);监控指标：\n\nzookeeper.znode.count 监控节点数量\nzookeeper.watch.count 监控Watcher数量\nzookeeper.avg_latency 监控集群延迟\n\n优点：\n\n🔒 强一致性：基于ZAB协议，保证分布式场景下锁状态一致性\n🚨 无死锁风险：临时节点自动删除（客户端断开即释放）\n🔍 精确唤醒：Watcher机制避免Redis方案中的无效轮询\n🛡️ 高可靠性：适合金融、交易等强一致性场景\n\n建议：\n\n对数据一致性要求极高的场景（如支付核心）选择ZooKeeper\n高并发读多写少场景（如商品查询）选择Redis方案\n关键业务建议双方案结合：ZooKeeper做主锁，Redis做缓存锁降级\n\n基于Redis核心机制解析加锁流程\n根据业务标识 key 查询 value\n如果 value 不存在说明锁没有被持有，可以直接加锁；\n如果 value 存在，且和自己的线程标识相等，则说明当线程前持锁；\n如果 value 存在，且和自己的线程标识不一样，说明锁正在被其它线程持有\n\n\n加锁需要创建一个代表业务标识的 key ，value 存放线程标识，需要设置过期时间来预防死锁\n创建一个 watchdog 后台运行，及时给锁续期\n\n解锁流程\n根据业务标识 key 查询 value\nvalue 和自己的线程标识一样，可以解锁\nvalue 和自己的线程标识不一样，说明已经自动解锁，并且锁被其它线程持有，无需解锁直接返回（有看门狗的存在，几乎不会发生这种情况）\n\n\n解锁需要删除自己的业务标识 key ，并且关闭自己开启的 watchdog\n\n工作流程\n实现方案Maven 依赖xml&lt;dependency&gt;\n    &lt;groupId&gt;org.redisson&lt;/groupId&gt;\n    &lt;artifactId&gt;redisson-spring-boot-starter&lt;/artifactId&gt;\n    &lt;version&gt;3.18.0&lt;/version&gt;\n&lt;/dependency&gt;配置类java@Data\n@ConfigurationProperties(prefix = &quot;lock.redisson&quot;)\npublic class RedissonLockProperties &#123;\n    public static final String REDIS_PROTO_PREFIX = &quot;redis://&quot;;\n\n    private RedisModel model = RedisModel.SINGLETON;\n    private String namespace = &quot;default&quot;;\n    private RSingleton singleton = new RSingleton();\n    private RSentinel sentinel = new RSentinel();\n    private RCluster cluster = new RCluster();\n\n    public enum RedisModel &#123;\n        SINGLETON, SENTINEL, CLUSTER\n    &#125;\n\n    @Data\n    public static class RSingleton &#123;\n        private String address = &quot;localhost:6379&quot;;\n        private String password;\n        private int timeout = 3000;\n        private int pingConnectionInterval = 60000;\n        private int connectionPoolSize = 64;\n        private int connectionMinimumIdleSize = 10;\n    &#125;\n\n    @Data\n    public static class RSentinel &#123;\n        private List&lt;String&gt; addresses = Collections.singletonList(&quot;localhost:6379&quot;);\n        /**\n         * 主节点名称\n         */\n        private String master;\n        private String password;\n        private int scanInterval = 2000;\n    &#125;\n\n    @Data\n    public static class RCluster &#123;\n        private List&lt;String&gt; addresses = Collections.singletonList(&quot;localhost:6379&quot;);\n        private String password;\n        private int scanInterval = 2000;\n    &#125;\n&#125;\njava@Configuration\n@EnableConfigurationProperties(RedissonLockProperties.class)\npublic class RedissonLockConfig &#123;\n\n    @Bean(destroyMethod = &quot;shutdown&quot;)\n    public RedissonClient redissonClient(RedissonLockProperties properties) &#123;\n        Config config = new Config();\n        switch (properties.getModel()) &#123;\n            case SENTINEL:\n                RedissonLockProperties.RSentinel sentinelProps = properties.getSentinel();\n                SentinelServersConfig sentinelServersConfig = config.useSentinelServers();\n                sentinelServersConfig.setMasterName(sentinelProps.getMaster());\n                List&lt;String&gt; addresses = sentinelProps.getAddresses().stream()\n                        .map(e -&gt; RedissonLockProperties.REDIS_PROTO_PREFIX + e)\n                        .collect(Collectors.toList());\n                sentinelServersConfig.setSentinelAddresses(addresses);\n                sentinelServersConfig.setScanInterval(sentinelProps.getScanInterval());\n                Optional.ofNullable(sentinelProps.getPassword())\n                        .ifPresent(sentinelServersConfig::setPassword);\n                break;\n            case CLUSTER:\n                ClusterServersConfig clusterServersConfig = config.useClusterServers();\n                RedissonLockProperties.RCluster clusterProps = properties.getCluster();\n                List&lt;String&gt; addr = clusterProps.getAddresses().stream()\n                        .map(e -&gt; RedissonLockProperties.REDIS_PROTO_PREFIX + e)\n                        .collect(Collectors.toList());\n                clusterServersConfig.setNodeAddresses(addr);\n                clusterServersConfig.setScanInterval(clusterProps.getScanInterval());\n                Optional.ofNullable(clusterProps.getPassword())\n                       .ifPresent(clusterServersConfig::setPassword);\n                break;\n            default:\n                RedissonLockProperties.RSingleton singletonProps = properties.getSingleton();\n                String address = RedissonLockProperties.REDIS_PROTO_PREFIX + singletonProps.getAddress();\n                SingleServerConfig singleServerConfig = config.useSingleServer()\n                        .setTimeout(singletonProps.getTimeout())\n                        .setPingConnectionInterval(singletonProps.getPingConnectionInterval())\n                        .setConnectionPoolSize(singletonProps.getConnectionPoolSize())\n                        .setConnectionMinimumIdleSize(singletonProps.getConnectionMinimumIdleSize())\n                        .setAddress(address).setDatabase(0);\n                Optional.ofNullable(singletonProps.getPassword())\n                        .ifPresent(singleServerConfig::setPassword);\n                break;\n        &#125;\n        return Redisson.create(config);\n    &#125;\n&#125;\n锁实现java@Component\npublic class RedissonLock &#123;\n    private final RedissonClient redissonClient;\n    private final String namespace;\n\n    public RedissonLock(RedissonClient redissonClient, RedissonLockProperties lockProps) &#123;\n        this.redissonClient = redissonClient;\n        this.namespace = lockProps.getNamespace();\n    &#125;\n\n    /**\n     * 尝试获取分布式锁\n     *\n     * @param lockKey   锁键\n     * @param waitTime  最大等待时间(秒)\n     * @param leaseTime 锁持有时间(秒)\n     * @return true=获取成功\n     */\n    public boolean tryLock(String lockKey, long waitTime, long leaseTime) &#123;\n        String key = namespace + &quot;:&quot; + lockKey;\n        RLock lock = redissonClient.getLock(key);\n        try &#123;\n            // 尝试加锁：支持等待时间、自动过期时间\n            return lock.tryLock(waitTime, leaseTime, TimeUnit.SECONDS);\n        &#125; catch (InterruptedException e) &#123;\n            Thread.currentThread().interrupt();\n            return false;\n        &#125;\n    &#125;\n\n    /**\n     * 释放分布式锁\n     *\n     * @param lockKey 锁键\n     */\n    public void unlock(String lockKey) &#123;\n        String key = namespace + &quot;:&quot; + lockKey;\n        RLock lock = redissonClient.getLock(key);\n        if (lock.isLocked() &amp;&amp; lock.isHeldByCurrentThread()) &#123;\n            lock.unlock();\n        &#125;\n    &#125;\n&#125;使用方法java@Service\n@RequiredArgsConstructor\npublic class Usage &#123;\n    private final RedissonLock lock;\n\n    private static final String ORDER_LOCK_PREFIX = &quot;order_lock:&quot;;\n\n    public Boolean createOrder(String orderId) &#123;\n        String lockKey = ORDER_LOCK_PREFIX + orderId;\n        try &#123;\n            if (lock.tryLock(lockKey, 3, 30)) &#123;\n                // 核心业务逻辑（保证订单ID幂等性）\n                Thread.sleep(1000);\n                return Boolean.TRUE;\n            &#125;\n            return Boolean.FALSE;\n        &#125; catch (InterruptedException e) &#123;\n            throw new RuntimeException(e);\n        &#125; finally &#123;\n            lock.unlock(lockKey);\n        &#125;\n    &#125;\n&#125;\nRedission 使用的是 hash 类型，key 是锁的名称，field 是客户端 ID，value 是该客户端加锁（可重入）的次数\nbash127.0.0.1:6379&gt; type default:order_lock:1000\nhash\n127.0.0.1:6379&gt; hgetall default:order_lock:1000\n1) &quot;7faee086-b466-45f7-9e98-361614e19e18:59&quot;\n2) &quot;1&quot;关键特性说明自动续期机制（看门狗）\njava// 当leaseTime参数为-1时，启动看门狗（默认30秒续期）\nlock.tryLock(3, -1, TimeUnit.SECONDS);红锁模式（应对Redis单点故障）\njavaConfig config1 = new Config();\nconfig1.useSingleServer().setAddress(&quot;redis://node1:6379&quot;);\n\nConfig config2 = new Config();\nconfig2.useSingleServer().setAddress(&quot;redis://node2:6379&quot;);\n\nRedissonClient client1 = Redisson.create(config1);\nRedissonClient client2 = Redisson.create(config2);\n\nRLock lock1 = client1.getLock(&quot;lock&quot;);\nRLock lock2 = client2.getLock(&quot;lock&quot;);\n\nRedissonRedLock redLock = new RedissonRedLock(lock1, lock2);\nredLock.lock();Redis方案适用于AP系统，满足99%高并发场景需求\n优点：\n\n⚡️ 高性能（内存操作，10万+ QPS）\n📦 原生支持键过期特性\n🔄 成熟的Redisson客户端提供看门狗自动续期\n🧩 支持红锁（RedLock）多实例部署模式\n\n基于数据库(MySQL)工作流程\n实现方案数据库设计sqlCREATE TABLE distributed_lock (\n    id BIGINT AUTO_INCREMENT PRIMARY KEY,\n    lock_key VARCHAR(128) NOT NULL COMMENT &#39;锁标识&#39;,\n    holder_id VARCHAR(64) NOT NULL COMMENT &#39;持有者标识&#39;,\n    expire_time DATETIME NOT NULL COMMENT &#39;过期时间&#39;,\n    version INT NOT NULL DEFAULT 0 COMMENT &#39;乐观锁版本&#39;,\n    UNIQUE INDEX uniq_lock_key(lock_key)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3;\n\n-- 初始数据示例\nINSERT INTO distributed_lock(lock_key, holder_id, expire_time) \nVALUES (&#39;order_lock&#39;, &#39;init&#39;, &#39;1970-01-01&#39;);锁实现java@Component\npublic class DatabaseLock &#123;\n\n    @Autowired\n    private JdbcTemplate jdbcTemplate;\n  \n    private ThreadLocal&lt;String&gt; holderId = new ThreadLocal&lt;&gt;(); // 实例唯一标识\n\n    /**\n     * 尝试获取分布式锁\n     * @param lockKey 锁名称\n     * @param expireSeconds 锁过期时间(秒)\n     * @param retryTimes 重试次数\n     * @param retryInterval 重试间隔(毫秒)\n     */\n    public boolean tryLock(String lockKey, int expireSeconds, int retryTimes, long retryInterval) &#123;\n        for (int i = 0; i &lt;= retryTimes; i++) &#123;\n            if (acquireLock(lockKey, expireSeconds)) &#123;\n                return true;\n            &#125;\n            try &#123;\n                Thread.sleep(retryInterval);\n            &#125; catch (InterruptedException e) &#123;\n                Thread.currentThread().interrupt();\n            &#125;\n        &#125;\n        return false;\n    &#125;\n\n    private boolean acquireLock(String lockKey, int expireSeconds) &#123;\n        holderId.set(UUID.randomUUID().toString());\n        LocalDateTime now = LocalDateTime.now();\n        LocalDateTime expireTime = now.plusSeconds(expireSeconds);\n        \n        // 1. 清理过期锁\n        jdbcTemplate.update(\n            &quot;DELETE FROM distributed_lock WHERE lock_key = ? AND expire_time &lt; ?&quot;,\n            lockKey, now\n        );\n        \n        // 2. 尝试插入新锁记录\n        try &#123;\n            jdbcTemplate.update(\n                &quot;INSERT INTO distributed_lock(lock_key, holder_id, expire_time) VALUES (?, ?, ?)&quot;,\n                lockKey, holderId.get(), expireTime\n            );\n            return true;\n        &#125; catch (DuplicateKeyException e) &#123;\n            // 3. 已有锁存在时尝试抢占\n            int updated = jdbcTemplate.update(\n                &quot;UPDATE distributed_lock SET holder_id = ?, expire_time = ?, version = version + 1 &quot; +\n                &quot;WHERE lock_key = ? AND (holder_id = ? OR expire_time &lt; ?)&quot;,\n                holderId.get(), expireTime, lockKey, holderId.get(), now\n            );\n            return updated &gt; 0;\n        &#125;\n    &#125;\n\n    /**\n     * 释放分布式锁\n     */\n    public void releaseLock(String lockKey) &#123;\n        jdbcTemplate.update(\n            &quot;DELETE FROM distributed_lock WHERE lock_key = ? AND holder_id = ?&quot;,\n            lockKey, holderId\n        );\n        holderId.remove();\n    &#125;\n&#125;使用方式java@Service\npublic class DatabaseLock &#123;\n\n    private static final String INVENTORY_LOCK = &quot;inventory_lock&quot;;\n\n    @Autowired\n    private DatabaseLock lock;\n\n    public Boolean reduceStock(skuId) &#123;\n        try &#123;\n            // 尝试获取锁（最多重试3次，每次间隔200ms）\n            if (lock.tryLock(INVENTORY_LOCK, 30, 3, 200)) &#123;\n                // 核心减库存逻辑\n                return Boolean.TRUE;\n            &#125;\n            return Boolean.FALSE;\n        &#125; finally &#123;\n            lock.releaseLock(INVENTORY_LOCK);\n        &#125;\n    &#125;\n&#125;关键机制解析双重保险机制：\nsql/* 抢占锁的SQL同时处理两种情况 */\nUPDATE ... \nWHERE lock_key = ? \n  AND (holder_id = ?  /* 当前持有者可续期 */\n       OR expire_time &lt; NOW() /* 过期锁可抢占 */\n  )死锁预防：\n\n通过expire_time字段保证锁最终释放\n\n后台定时任务清理过期锁：\njava@Scheduled(fixedRate = 60000) // 每分钟清理一次\npublic void cleanExpiredLocks() &#123;\n    jdbcTemplate.update(\n        &quot;DELETE FROM distributed_lock WHERE expire_time &lt; ?&quot;,\n        LocalDateTime.now().minusMinutes(5)\n    );\n&#125;\n\n锁续期方案\njavapublic void renewLock(String lockKey, int expireSeconds) &#123;\n    jdbcTemplate.update(\n        &quot;UPDATE distributed_lock SET expire_time = ? &quot; +\n        &quot;WHERE lock_key = ? AND holder_id = ?&quot;,\n        LocalDateTime.now().plusSeconds(expireSeconds),\n        lockKey, holderId\n    );\n&#125;分库分表：\njava// 根据 holderId 哈希分库 8个库\nString db = &quot;db_&quot; + (holderId.hashCode() &amp; 7);\n// 根据 lock_key 哈希分表 每库 16 个表\nString table = &quot;distributed_lock_&quot; + (lockKey.hashCode() &amp; 15);总结没有最好的方案，只有最适合的方案，不要过度设计，一切选择要基于实际需求，最终我们还是选择了 Zookeeper 的方案\n\n我们的保险业务没有高并发的场景\n我们的保险业务需要强一致性\n\n","slug":"分布式锁的三种实现方案","date":"2017-08-05T05:04:29.000Z","categories_index":"分布式","tags_index":"MySQL,Zookeeper,Redis,分布式,分布式锁","author_index":"GlassCat"}]